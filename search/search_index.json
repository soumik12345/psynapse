{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Psynapse","text":"<p>A visual node-based workflow editor for creating and executing computational graphs with Python. Psynapse is meant to be the single no-code workflow editor that lets you harness the entire power of the python ecosystem using simple nodepacks and intuitive drag-and-drop interface.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Visual Node Editor: Drag-and-drop interface built with ReactFlow</li> <li>Python Backend: FastAPI server with real-time execution streaming</li> <li>Extensible Nodepacks: Add custom Python functions as nodes</li> <li>Type-Safe: Full TypeScript frontend and type-hinted Python backend</li> <li>Progress Tracking: Real-time progress updates for long-running operations</li> <li>Multiple Node Types: Functions, variables, lists, and view nodes</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#using-docker-compose-recommended","title":"Using Docker Compose (Recommended)","text":"BasicWith LLM SupportWith Pytorch GPU Backend <pre><code>docker compose -f docker/docker-compose.yml up --build\n</code></pre> <pre><code>OPTIONAL_DEPS=llm docker compose -f docker/docker-compose.yml up --build\n</code></pre> <pre><code>OPTIONAL_DEPS=llm,z-image docker compose -f docker/docker-compose-torch-gpu.yml up --build\n</code></pre> <p>Access the editor at <code>http://localhost:5173</code></p>"},{"location":"#local-development","title":"Local Development","text":"<p>Backend: <pre><code>uv sync\npsynapse-backend run --reload\n</code></pre></p> <p>Frontend: <pre><code>cd frontend\nnpm install\nnpm run dev\n</code></pre></p>"},{"location":"#creating-custom-nodepacks","title":"Creating Custom Nodepacks","text":"<p>Add a new directory in <code>nodepacks/</code> with an <code>ops.py</code> file:</p> <pre><code>def my_function(text: str, count: int = 1) -&gt; str:\n    \"\"\"Repeats text multiple times.\"\"\"\n    return text * count\n</code></pre> <p>Restart the backend to auto-register your nodes.</p>"},{"location":"backend/executor/","title":"Graph Executor","text":"<p>The Graph Executor is responsible for executing node graphs in the correct order with dependency resolution.</p>"},{"location":"backend/executor/#features","title":"Features","text":"<ul> <li>Topological Sorting: Uses Kahn's algorithm to determine execution order</li> <li>Function Execution: Executes regular Python functions from <code>ops.py</code></li> <li>Progress Node Execution: Executes progress-aware classes from <code>progress_ops.py</code> with real-time progress reporting</li> <li>Streaming Execution: Provides real-time status updates via Server-Sent Events</li> <li>Error Handling: Gracefully handles execution errors and reports them</li> </ul>"},{"location":"backend/executor/#progress-node-support","title":"Progress Node Support","text":"<p>Progress nodes are classes with <code>__call__</code> methods that can report progress during execution. The executor:</p> <ol> <li>Detects progress nodes by checking the <code>progress_class_registry</code></li> <li>Instantiates the class</li> <li>Sets up a progress callback on the <code>_progress_reporter</code> attribute</li> <li>Executes the <code>__call__</code> method in a separate thread</li> <li>Streams progress updates via SSE during execution</li> </ol> <p>See the Progress Nodes Guide for details on creating progress nodes.</p>"},{"location":"backend/executor/#api-reference","title":"API Reference","text":""},{"location":"backend/executor/#psynapse_backend.executor","title":"<code>psynapse_backend.executor</code>","text":""},{"location":"backend/executor/#psynapse_backend.executor.GraphExecutor","title":"<code>GraphExecutor</code>","text":"<p>GraphExecutor is responsible for executing the node graph. The graph is executed in the correct order with dependency resolution by topological sorting using Kahn's algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>nodepacks_dir</code> <code>str</code> <p>The directory containing the nodepacks.</p> required <p>Attributes:</p> Name Type Description <code>nodepacks_dir</code> <p>The directory containing the nodepacks.</p> <code>function_registry</code> <p>A dictionary of functions from the nodepacks.</p> <code>progress_class_registry</code> <p>A dictionary of progress classes from the nodepacks.</p> <code>stream_class_registry</code> <p>A dictionary of stream classes from the nodepacks.</p>"},{"location":"backend/executor/#psynapse_backend.executor.GraphExecutor.topological_sort","title":"<code>topological_sort(nodes, edges)</code>","text":"<p>Perform topological sort using Kahn's algorithm. Returns list of node IDs in execution order.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[dict]</code> <p>The nodes of the graph.</p> required <code>edges</code> <code>list[dict]</code> <p>The edges of the graph.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of node IDs in execution order.</p>"},{"location":"backend/executor/#psynapse_backend.executor.GraphExecutor.execute_graph","title":"<code>execute_graph(nodes, edges, env_vars=None)</code>","text":"<p>Execute the node graph and return results for <code>ViewNodes</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[dict]</code> <p>The nodes of the graph.</p> required <code>edges</code> <code>list[dict]</code> <p>The edges of the graph.</p> required <code>env_vars</code> <code>dict[str, str] | None</code> <p>Optional environment variables to set during execution.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of node IDs and their results.</p>"},{"location":"backend/executor/#psynapse_backend.executor.GraphExecutor.execute_graph_streaming","title":"<code>execute_graph_streaming(nodes, edges, env_vars=None)</code>","text":"<p>Execute the node graph and yield status updates for each node.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[dict]</code> <p>The nodes of the graph.</p> required <code>edges</code> <code>list[dict]</code> <p>The edges of the graph.</p> required <code>env_vars</code> <code>dict[str, str] | None</code> <p>Optional environment variables to set during execution.</p> <code>None</code> <p>Yields:</p> Type Description <code>Any</code> <p>Status dictionaries containing execution progress information.</p>"},{"location":"backend/schema_extractor/","title":"Schema Extractor","text":"<p>The Schema Extractor dynamically discovers and extracts metadata from Python functions and classes in nodepacks.</p>"},{"location":"backend/schema_extractor/#features","title":"Features","text":"<ul> <li>Function Discovery: Extracts schemas from functions in <code>ops.py</code> files</li> <li>Progress Class Discovery: Extracts schemas from classes with <code>__call__</code> methods in <code>progress_ops.py</code> files</li> <li>Type Hint Parsing: Extracts type information from function signatures and annotations</li> <li>Docstring Extraction: Captures documentation strings for nodes</li> </ul>"},{"location":"backend/schema_extractor/#progress-node-support","title":"Progress Node Support","text":"<p>The schema extractor scans <code>progress_ops.py</code> files for classes that: - Have a <code>__call__</code> method - Are not private (don't start with <code>_</code>) - Are defined at module level</p> <p>Progress node schemas include an <code>is_progress_node: true</code> flag to distinguish them from regular function nodes.</p> <p>See the Progress Nodes Guide for details on creating progress nodes.</p>"},{"location":"backend/schema_extractor/#api-reference","title":"API Reference","text":""},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor","title":"<code>psynapse_backend.schema_extractor</code>","text":""},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.AnnotatedDict","title":"<code>AnnotatedDict</code>","text":"<p>               Bases: <code>dict</code>, <code>Generic[T]</code></p> <p>A dictionary type hint that specifies the expected output keys.</p> Usage <p>def my_func() -&gt; AnnotatedDict[Literal[\"key1\", \"key2\"]]:     return {\"key1\": value1, \"key2\": value2}</p> <p>This creates multiple output handles in the node editor, one for each key.</p>"},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.get_type_name","title":"<code>get_type_name(type_hint)</code>","text":"<p>Convert a type hint to a string representation.</p> <p>Parameters:</p> Name Type Description Default <code>type_hint</code> <code>Any</code> <p>The type hint to convert.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the type hint.</p>"},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.get_literal_values","title":"<code>get_literal_values(type_hint)</code>","text":"<p>Extract literal values from a Literal type hint.</p> <p>Parameters:</p> Name Type Description Default <code>type_hint</code> <code>Any</code> <p>The type hint to check.</p> required <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of string values if the type hint is Literal, None otherwise.</p>"},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.parse_annotated_dict_keys","title":"<code>parse_annotated_dict_keys(type_hint)</code>","text":"<p>Parse AnnotatedDict[Literal['key1', 'key2', ...]] and return the key names.</p> <p>Parameters:</p> Name Type Description Default <code>type_hint</code> <code>Any</code> <p>The type hint to check.</p> required <p>Returns:</p> Type Description <code>list[str] | None</code> <p>A list of key names if the type hint is AnnotatedDict with Literal keys, None otherwise.</p>"},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.extract_function_schema","title":"<code>extract_function_schema(func, filepath)</code>","text":"<p>Extract schema information from a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to extract schema information from.</p> required <code>filepath</code> <code>str</code> <p>The file path of the function.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of schema information.</p>"},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.detect_class_node_type","title":"<code>detect_class_node_type(cls)</code>","text":"<p>Detect the node type of a class based on its attributes.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The class to inspect.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>\"progress\" if the class has _progress_reporter,</p> <code>str | None</code> <p>\"stream\" if the class has _stream_reporter,</p> <code>str | None</code> <p>None otherwise.</p>"},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.extract_class_schema","title":"<code>extract_class_schema(cls, filepath, node_type=None)</code>","text":"<p>Extract schema information from a class with call method.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The class to extract schema information from.</p> required <code>filepath</code> <code>str</code> <p>The file path of the class.</p> required <code>node_type</code> <code>str | None</code> <p>The type of node (\"progress\", \"stream\", or None for auto-detection).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of schema information.</p>"},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.extract_schemas_from_file","title":"<code>extract_schemas_from_file(filepath, extract_classes=False, node_type='progress')</code>","text":"<p>Extract schemas from all functions or classes in a Python file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The file path of the Python file.</p> required <code>extract_classes</code> <code>bool</code> <p>If True, extract classes with call method. If False, extract functions.</p> <code>False</code> <code>node_type</code> <code>str</code> <p>The type of node for classes (\"progress\" or \"stream\").</p> <code>'progress'</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries of schema information.</p>"},{"location":"backend/schema_extractor/#psynapse_backend.schema_extractor.extract_all_schemas","title":"<code>extract_all_schemas(nodepacks_dir)</code>","text":"<p>Extract schemas from all ops.py, progress_ops.py, and stream_ops.py files in the nodepacks directory.</p> <p>Parameters:</p> Name Type Description Default <code>nodepacks_dir</code> <code>str</code> <p>The directory containing the nodepacks.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>A list of dictionaries of schema information.</p>"},{"location":"examples/llm/","title":"LLM Workflow Examples","text":""},{"location":"examples/llm/#openai-chat-completion","title":"OpenAI Chat Completion","text":""},{"location":"examples/llm/#simple-openai-respose","title":"Simple OpenAI Respose","text":"<p>Workflow File</p>"},{"location":"examples/llm/#image-prompts","title":"Image Prompts","text":"<p>Workflow File</p>"},{"location":"examples/llm/#openai-streaming","title":"OpenAI Streaming","text":"<p>Workflow File</p>"},{"location":"frontend/components/","title":"Components","text":""},{"location":"frontend/components/#psynapseeditor","title":"PsynapseEditor","text":"<p>The main editor component that provides the visual node-based programming interface.</p>"},{"location":"frontend/components/#description","title":"Description","text":"<p><code>PsynapseEditor</code> is the root component of the visual editor. It wraps the editor implementation in a <code>ReactFlowProvider</code> to enable React Flow functionality. The actual implementation is in <code>PsynapseEditorInner</code>.</p>"},{"location":"frontend/components/#features","title":"Features","text":"<ul> <li>Drag-and-drop interface: Add nodes from the library panel by dragging them onto the canvas</li> <li>Visual node connections: Connect nodes by dragging edges between input and output handles</li> <li>Graph execution: Execute the entire node graph with a single button click</li> <li>Real-time updates: View nodes update automatically after graph execution</li> <li>Mini-map and controls: Built-in navigation aids for large graphs</li> </ul>"},{"location":"frontend/components/#state-management","title":"State Management","text":"<p>The component manages several pieces of state:</p> <ul> <li><code>nodes</code>: Array of nodes in the graph</li> <li><code>edges</code>: Array of connections between nodes</li> <li><code>reactFlowInstance</code>: Reference to the React Flow instance</li> <li><code>executing</code>: Boolean indicating whether graph execution is in progress</li> <li><code>statusHistory</code>: Array of execution status updates for real-time display</li> <li><code>abortExecution</code>: Cleanup function to abort ongoing execution</li> </ul>"},{"location":"frontend/components/#key-methods","title":"Key Methods","text":""},{"location":"frontend/components/#onconnect","title":"<code>onConnect</code>","text":"<p>Handles new edge connections between nodes. Updates both the edges state and propagates edge information to all nodes.</p> <p>Parameters: - <code>connection: Connection</code> - The new connection to add</p>"},{"location":"frontend/components/#ondrop","title":"<code>onDrop</code>","text":"<p>Handles dropping nodes from the library panel onto the canvas. Creates new nodes based on the dropped data.</p> <p>Parameters: - <code>event: React.DragEvent</code> - The drop event</p> <p>Supports: - <code>viewNode</code>: Creates a view node for displaying results - <code>functionNode</code>: Creates a function node with parameters based on schema</p>"},{"location":"frontend/components/#handlenodedatachange","title":"<code>handleNodeDataChange</code>","text":"<p>Updates node data when parameter values change.</p> <p>Parameters: - <code>nodeId: string</code> - The ID of the node to update - <code>paramName: string</code> - The name of the parameter to update - <code>value: string</code> - The new value for the parameter</p>"},{"location":"frontend/components/#executegraph","title":"<code>executeGraph</code>","text":"<p>Executes the entire node graph using real-time streaming from the backend API.</p> <p>Process: 1. Clears previous execution status history 2. Prepares nodes and edges for execution 3. Sends graph to backend via <code>api.executeGraphStreaming()</code> 4. Receives real-time status updates via Server-Sent Events (SSE) 5. Updates status history as nodes execute 6. Updates view nodes with final execution results 7. Handles errors with user feedback</p> <p>Callbacks: - <code>onStatus(status: ExecutionStatus)</code>: Called for each node execution update - <code>onComplete(results)</code>: Called when execution finishes successfully - <code>onError(error)</code>: Called if execution fails</p>"},{"location":"frontend/components/#usage-example","title":"Usage Example","text":"<pre><code>import PsynapseEditor from './components/PsynapseEditor';\n\nfunction App() {\n  return &lt;PsynapseEditor /&gt;;\n}\n</code></pre>"},{"location":"frontend/components/#functionnode","title":"FunctionNode","text":"<p>A node component representing a Python function with input parameters and output.</p>"},{"location":"frontend/components/#props","title":"Props","text":"<pre><code>interface NodeProps&lt;NodeData&gt; {\n  data: NodeData;\n  id: string;\n}\n</code></pre> <p>NodeData includes: - <code>label: string</code> - The display name of the function - <code>params?: ParamSchema[]</code> - Array of parameter schemas - <code>onChange?: (nodeId: string, paramName: string, value: string) =&gt; void</code> - Callback for parameter changes - <code>edges?: any[]</code> - Current graph edges</p>"},{"location":"frontend/components/#features_1","title":"Features","text":"<ul> <li>Dynamic inputs: Creates input fields based on function schema</li> <li>Connection awareness: Hides input fields when parameters are connected via edges</li> <li>Type information: Displays parameter types next to labels</li> <li>Visual feedback: Shows connection status with colored indicators</li> </ul>"},{"location":"frontend/components/#styling","title":"Styling","text":"<ul> <li>Border: Blue (<code>#4a90e2</code>)</li> <li>Background: White</li> <li>Handles: Blue circles for input/output connections</li> </ul>"},{"location":"frontend/components/#input-behavior","title":"Input Behavior","text":"<p>Each parameter can be: - Connected: Input field hidden, shows connection indicator (\u25cf) - Unconnected: Text input field visible for manual value entry</p>"},{"location":"frontend/components/#usage-example_1","title":"Usage Example","text":"<pre><code>const nodeTypes = {\n  functionNode: FunctionNode,\n};\n\n&lt;ReactFlow nodeTypes={nodeTypes} ... /&gt;\n</code></pre>"},{"location":"frontend/components/#viewnode","title":"ViewNode","text":"<p>A node component for displaying execution results.</p>"},{"location":"frontend/components/#props_1","title":"Props","text":"<pre><code>interface NodeProps&lt;NodeData&gt; {\n  data: NodeData;\n}\n</code></pre> <p>NodeData includes: - <code>value?: any</code> - The value to display (updated after execution)</p>"},{"location":"frontend/components/#features_2","title":"Features","text":"<ul> <li>Result display: Shows execution results in a monospace font</li> <li>Default message: Displays \"No value\" when no result is available</li> <li>Single input: Accepts one incoming connection</li> </ul>"},{"location":"frontend/components/#styling_1","title":"Styling","text":"<ul> <li>Border: Green (<code>#50c878</code>)</li> <li>Background: Light green (<code>#f0fdf4</code>)</li> <li>Content box: White with monospace font</li> </ul>"},{"location":"frontend/components/#display-behavior","title":"Display Behavior","text":"<ul> <li>Converts any value to string for display</li> <li>Handles undefined values gracefully</li> <li>Word-wrapping for long values</li> </ul>"},{"location":"frontend/components/#usage-example_2","title":"Usage Example","text":"<pre><code>const nodeTypes = {\n  viewNode: ViewNode,\n};\n\n&lt;ReactFlow nodeTypes={nodeTypes} ... /&gt;\n</code></pre>"},{"location":"frontend/components/#nodelibrarypanel","title":"NodeLibraryPanel","text":"<p>A sidebar panel displaying available nodes that can be dragged onto the canvas.</p>"},{"location":"frontend/components/#props_2","title":"Props","text":"<pre><code>interface NodeLibraryPanelProps {\n  schemas: FunctionSchema[];\n  loading: boolean;\n  error: string | null;\n}\n</code></pre>"},{"location":"frontend/components/#features_3","title":"Features","text":"<ul> <li>Built-in nodes section: Contains ViewNode</li> <li>Function nodes section: Dynamically generated from backend schemas</li> <li>Drag-and-drop: All nodes are draggable onto the canvas</li> <li>Loading state: Shows loading indicator while fetching schemas</li> <li>Error handling: Displays error messages if schema fetch fails</li> </ul>"},{"location":"frontend/components/#node-information-display","title":"Node Information Display","text":"<p>For each function node, displays: - Function name - Number of inputs and outputs (e.g., \"2 inputs \u2192 1 outputs\")</p>"},{"location":"frontend/components/#drag-data-format","title":"Drag Data Format","text":"<p>Sends data in <code>application/reactflow</code> format:</p> <p>For ViewNode: <pre><code>{ type: 'viewNode' }\n</code></pre></p> <p>For FunctionNode: <pre><code>{ \n  type: 'functionNode', \n  schema: FunctionSchema \n}\n</code></pre></p>"},{"location":"frontend/components/#styling_2","title":"Styling","text":"<ul> <li>Panel width: 260px</li> <li>Background: Light gray (<code>#f8f9fa</code>)</li> <li>Node items: White cards with hover effects</li> </ul>"},{"location":"frontend/components/#usage-example_3","title":"Usage Example","text":"<pre><code>&lt;NodeLibraryPanel \n  schemas={functionSchemas}\n  loading={isLoading}\n  error={errorMessage}\n/&gt;\n</code></pre>"},{"location":"frontend/components/#statuspanel","title":"StatusPanel","text":"<p>A real-time execution status panel that displays node execution progress during graph execution.</p>"},{"location":"frontend/components/#props_3","title":"Props","text":"<pre><code>interface StatusPanelProps {\n  statusHistory: ExecutionStatus[];\n}\n</code></pre>"},{"location":"frontend/components/#features_4","title":"Features","text":"<ul> <li>Real-time updates: Shows node execution status as it happens via Server-Sent Events (SSE)</li> <li>Execution history: Displays all nodes executed in topological order</li> <li>Node information: Shows node number, name, and status</li> <li>Spinner animation: Displays animated spinner for nodes currently executing</li> <li>Progress bars: Visual progress indicators for progress-aware nodes with percentage and messages</li> <li>Collapsible inputs: Expandable section showing input parameters</li> <li>Output display: Shows execution results for completed nodes</li> <li>Error handling: Displays error messages for failed nodes</li> <li>Color coding: Visual status indicators (blue=executing/progress, green=completed, red=error)</li> </ul>"},{"location":"frontend/components/#status-information-display","title":"Status Information Display","text":"<p>For each node in the execution history: - Node number badge: Sequential number (1-indexed) with color-coded background - Node name: Function name or node label - Status icon: Spinner (executing/progress), checkmark (completed), or X (error) - Progress bar: For progress nodes, displays:   - Horizontal progress bar showing completion percentage (0-100%)   - Progress message (e.g., \"Processing item 3/10\")   - Percentage text display   - Smooth transitions as progress updates - Inputs section: Collapsible display of input parameters and values - Output section: Result value displayed after completion - Error section: Error message if execution failed</p>"},{"location":"frontend/components/#styling_3","title":"Styling","text":"<ul> <li>Panel width: 400px</li> <li>Position: Fixed on right side of screen</li> <li>Background: Light gray (<code>#f8f9fa</code>)</li> <li>Node cards: White with colored borders matching status</li> <li>Status colors:</li> <li>Executing/Progress: Blue (<code>#007bff</code>)</li> <li>Completed: Green (<code>#28a745</code>)</li> <li>Error: Red (<code>#dc3545</code>)</li> <li>Progress bar styling:</li> <li>Background: Light gray (<code>#e9ecef</code>)</li> <li>Fill: Blue (<code>#007bff</code>)</li> <li>Height: 8px with rounded corners</li> <li>Smooth transitions: 0.3s ease</li> </ul>"},{"location":"frontend/components/#state-behavior","title":"State Behavior","text":"<p>The panel displays nodes in the order they are executed: 1. When a node starts executing, it appears with a spinner 2. For progress nodes, progress updates replace the executing status with progress bars 3. Progress bars update in real-time as the node reports progress 4. When execution completes, the node updates with output 5. If execution fails, the node shows error details 6. All executed nodes remain visible in a scrollable history</p>"},{"location":"frontend/components/#progress-node-support","title":"Progress Node Support","text":"<p>Progress nodes display special progress indicators: - Progress status: Status is set to <code>\"progress\"</code> during execution - Progress bar: Visual bar showing completion percentage - Progress message: Custom message from the node (e.g., \"Processing item 5/10\") - Real-time updates: Progress bar updates smoothly as progress is reported - Completion: Progress bar is replaced with output when execution completes</p> <p>See the Progress Nodes Guide for details on creating progress-aware nodes.</p>"},{"location":"frontend/components/#value-rendering","title":"Value Rendering","text":"<p>The panel intelligently renders different value types: - Primitives: Displayed as strings - Objects/Arrays: JSON.stringify with formatting - Null/undefined: Displayed as \"null\"</p>"},{"location":"frontend/components/#usage-example_4","title":"Usage Example","text":"<pre><code>&lt;StatusPanel statusHistory={executionStatusHistory} /&gt;\n</code></pre>"},{"location":"frontend/components/#integration","title":"Integration","text":"<p>The StatusPanel is integrated into PsynapseEditor and receives real-time updates through the streaming execution API:</p> <pre><code>const [statusHistory, setStatusHistory] = useState&lt;ExecutionStatus[]&gt;([]);\n\n// During execution, status updates are appended to history\napi.executeGraphStreaming(\n  request,\n  (status) =&gt; {\n    setStatusHistory(prev =&gt; {\n      // Update logic for managing status history\n    });\n  },\n  onComplete,\n  onError\n);\n</code></pre>"},{"location":"frontend/hooks/","title":"Hooks","text":""},{"location":"frontend/hooks/#useschema","title":"useSchema","text":"<p>A custom React hook for fetching and managing function schemas from the backend API.</p>"},{"location":"frontend/hooks/#signature","title":"Signature","text":"<pre><code>const useSchema = () =&gt; {\n  schemas: FunctionSchema[];\n  loading: boolean;\n  error: string | null;\n}\n</code></pre>"},{"location":"frontend/hooks/#return-value","title":"Return Value","text":"<p>Returns an object with three properties:</p> Property Type Description <code>schemas</code> <code>FunctionSchema[]</code> Array of function schemas loaded from the backend <code>loading</code> <code>boolean</code> Indicates whether schemas are currently being fetched <code>error</code> <code>string \\| null</code> Error message if fetch failed, null otherwise"},{"location":"frontend/hooks/#behavior","title":"Behavior","text":"<p>The hook automatically fetches schemas when the component mounts using the <code>useEffect</code> hook:</p> <ol> <li>Sets <code>loading</code> to <code>true</code> at the start of the fetch</li> <li>Calls <code>api.getSchemas()</code> to retrieve schemas from the backend</li> <li>Updates <code>schemas</code> state with the fetched data on success</li> <li>Sets <code>error</code> state with error message on failure</li> <li>Sets <code>loading</code> to <code>false</code> when complete (success or failure)</li> </ol>"},{"location":"frontend/hooks/#error-handling","title":"Error Handling","text":"<p>Errors are caught and handled gracefully: - If the error is an <code>Error</code> instance, uses its message - Otherwise, uses a generic \"Failed to fetch schemas\" message - Errors are also logged to the console for debugging</p>"},{"location":"frontend/hooks/#usage-example","title":"Usage Example","text":"<pre><code>import { useSchema } from '../hooks/useSchema';\n\nfunction MyComponent() {\n  const { schemas, loading, error } = useSchema();\n\n  if (loading) {\n    return &lt;div&gt;Loading schemas...&lt;/div&gt;;\n  }\n\n  if (error) {\n    return &lt;div&gt;Error: {error}&lt;/div&gt;;\n  }\n\n  return (\n    &lt;div&gt;\n      {schemas.map(schema =&gt; (\n        &lt;div key={schema.name}&gt;{schema.name}&lt;/div&gt;\n      ))}\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"frontend/hooks/#dependencies","title":"Dependencies","text":"<ul> <li>Internal: <code>api.getSchemas()</code> from <code>../utils/api</code></li> <li>External: React's <code>useState</code> and <code>useEffect</code> hooks</li> <li>Types: <code>FunctionSchema</code> from <code>../types/schema</code></li> </ul>"},{"location":"frontend/hooks/#state-updates","title":"State Updates","text":"<p>The hook manages three pieces of state internally:</p> <ol> <li>schemas: Initially an empty array <code>[]</code></li> <li>loading: Initially <code>true</code></li> <li>error: Initially <code>null</code></li> </ol>"},{"location":"frontend/hooks/#when-to-use","title":"When to Use","text":"<p>Use this hook when you need to: - Display available function nodes in the UI - Populate a node library or selection menu - Validate available functions before graph creation - Show function metadata to users</p>"},{"location":"frontend/hooks/#limitations","title":"Limitations","text":"<ul> <li>Fetches schemas only once on mount (no refetch mechanism)</li> <li>No caching between component unmounts</li> <li>No manual refresh capability</li> <li>Assumes API endpoint is available at component mount time</li> </ul>"},{"location":"frontend/types/","title":"Type Definitions","text":"<p>Type definitions for the Psynapse frontend application.</p>"},{"location":"frontend/types/#function-schema-types","title":"Function Schema Types","text":""},{"location":"frontend/types/#paramschema","title":"ParamSchema","text":"<p>Describes a function parameter.</p> <pre><code>interface ParamSchema {\n  name: string;\n  type: string;\n}\n</code></pre>"},{"location":"frontend/types/#properties","title":"Properties","text":"Property Type Description <code>name</code> <code>string</code> The parameter name <code>type</code> <code>string</code> The parameter type (e.g., \"int\", \"str\", \"float\")"},{"location":"frontend/types/#example","title":"Example","text":"<pre><code>const param: ParamSchema = {\n  name: \"x\",\n  type: \"int\"\n};\n</code></pre>"},{"location":"frontend/types/#returnschema","title":"ReturnSchema","text":"<p>Describes a function return value.</p> <pre><code>interface ReturnSchema {\n  name: string;\n  type: string;\n}\n</code></pre>"},{"location":"frontend/types/#properties_1","title":"Properties","text":"Property Type Description <code>name</code> <code>string</code> The return value name <code>type</code> <code>string</code> The return value type"},{"location":"frontend/types/#example_1","title":"Example","text":"<pre><code>const returnVal: ReturnSchema = {\n  name: \"result\",\n  type: \"int\"\n};\n</code></pre>"},{"location":"frontend/types/#functionschema","title":"FunctionSchema","text":"<p>Complete schema for a Python function exposed to the visual editor.</p> <pre><code>interface FunctionSchema {\n  name: string;\n  params: ParamSchema[];\n  returns: ReturnSchema[];\n  docstring: string;\n  filepath: string;\n}\n</code></pre>"},{"location":"frontend/types/#properties_2","title":"Properties","text":"Property Type Description <code>name</code> <code>string</code> The function name <code>params</code> <code>ParamSchema[]</code> Array of parameter schemas <code>returns</code> <code>ReturnSchema[]</code> Array of return value schemas <code>docstring</code> <code>string</code> The function's documentation string <code>filepath</code> <code>string</code> Path to the source file containing the function"},{"location":"frontend/types/#example_2","title":"Example","text":"<pre><code>const schema: FunctionSchema = {\n  name: \"add\",\n  params: [\n    { name: \"a\", type: \"int\" },\n    { name: \"b\", type: \"int\" }\n  ],\n  returns: [\n    { name: \"sum\", type: \"int\" }\n  ],\n  docstring: \"Adds two integers together\",\n  filepath: \"/path/to/ops.py\"\n};\n</code></pre>"},{"location":"frontend/types/#node-data-types","title":"Node Data Types","text":""},{"location":"frontend/types/#nodedata","title":"NodeData","text":"<p>Data structure for node instances in the graph.</p> <pre><code>interface NodeData {\n  label: string;\n  functionName?: string;\n  params?: ParamSchema[];\n  [key: string]: any;\n}\n</code></pre>"},{"location":"frontend/types/#properties_3","title":"Properties","text":"Property Type Required Description <code>label</code> <code>string</code> Yes Display name for the node <code>functionName</code> <code>string</code> No Name of the function (for function nodes) <code>params</code> <code>ParamSchema[]</code> No Parameter schemas (for function nodes) <code>[key: string]</code> <code>any</code> No Dynamic properties for parameter values, edges, callbacks, etc."},{"location":"frontend/types/#common-dynamic-properties","title":"Common Dynamic Properties","text":"<ul> <li><code>edges</code>: Current graph edges</li> <li><code>onChange</code>: Callback for parameter changes</li> <li><code>value</code>: Result value (for view nodes)</li> <li>Parameter values (e.g., <code>x: \"5\"</code>, <code>y: \"10\"</code>)</li> </ul>"},{"location":"frontend/types/#example_3","title":"Example","text":"<pre><code>const functionNodeData: NodeData = {\n  label: \"Add\",\n  functionName: \"add\",\n  params: [\n    { name: \"a\", type: \"int\" },\n    { name: \"b\", type: \"int\" }\n  ],\n  a: \"5\",\n  b: \"10\",\n  edges: [],\n  onChange: (nodeId, param, value) =&gt; { /* ... */ }\n};\n\nconst viewNodeData: NodeData = {\n  label: \"View\",\n  value: 15,\n  edges: []\n};\n</code></pre>"},{"location":"frontend/types/#api-requestresponse-types","title":"API Request/Response Types","text":""},{"location":"frontend/types/#executerequest","title":"ExecuteRequest","text":"<p>Request structure for graph execution.</p> <pre><code>interface ExecuteRequest {\n  nodes: any[];\n  edges: any[];\n}\n</code></pre>"},{"location":"frontend/types/#properties_4","title":"Properties","text":"Property Type Description <code>nodes</code> <code>any[]</code> Array of node objects with id, type, and data <code>edges</code> <code>any[]</code> Array of edge objects with source, target, and handle information"},{"location":"frontend/types/#node-structure","title":"Node Structure","text":"<pre><code>{\n  id: string;\n  type: string;\n  data: NodeData;\n}\n</code></pre>"},{"location":"frontend/types/#edge-structure","title":"Edge Structure","text":"<pre><code>{\n  source: string;\n  target: string;\n  sourceHandle: string;\n  targetHandle: string;\n}\n</code></pre>"},{"location":"frontend/types/#example_4","title":"Example","text":"<pre><code>const request: ExecuteRequest = {\n  nodes: [\n    {\n      id: \"node_1\",\n      type: \"functionNode\",\n      data: {\n        label: \"Add\",\n        functionName: \"add\",\n        a: \"5\",\n        b: \"10\"\n      }\n    },\n    {\n      id: \"node_2\",\n      type: \"viewNode\",\n      data: {\n        label: \"View\"\n      }\n    }\n  ],\n  edges: [\n    {\n      source: \"node_1\",\n      target: \"node_2\",\n      sourceHandle: \"output\",\n      targetHandle: \"input\"\n    }\n  ]\n};\n</code></pre>"},{"location":"frontend/types/#executeresponse","title":"ExecuteResponse","text":"<p>Response structure from graph execution.</p> <pre><code>interface ExecuteResponse {\n  results: { [nodeId: string]: any };\n}\n</code></pre>"},{"location":"frontend/types/#properties_5","title":"Properties","text":"Property Type Description <code>results</code> <code>{ [nodeId: string]: any }</code> Map of node IDs to their computed values"},{"location":"frontend/types/#example_5","title":"Example","text":"<pre><code>const response: ExecuteResponse = {\n  results: {\n    \"node_1\": 15,\n    \"node_2\": 15\n  }\n};\n</code></pre>"},{"location":"frontend/types/#executionstatus","title":"ExecutionStatus","text":"<p>Status information for a node during graph execution, used for real-time execution monitoring.</p> <pre><code>interface ExecutionStatus {\n  node_id: string;\n  node_number: number;\n  node_name: string;\n  status: 'executing' | 'completed' | 'error';\n  inputs?: Record&lt;string, any&gt;;\n  output?: any;\n  error?: string;\n}\n</code></pre>"},{"location":"frontend/types/#properties_6","title":"Properties","text":"Property Type Required Description <code>node_id</code> <code>string</code> Yes The unique identifier of the node <code>node_number</code> <code>number</code> Yes Sequential execution number (1-indexed) <code>node_name</code> <code>string</code> Yes Display name of the node or function <code>status</code> <code>'executing' \\| 'completed' \\| 'error'</code> Yes Current execution status <code>inputs</code> <code>Record&lt;string, any&gt;</code> No Input parameters and their values <code>output</code> <code>any</code> No Result value (present when status is 'completed') <code>error</code> <code>string</code> No Error message (present when status is 'error')"},{"location":"frontend/types/#status-values","title":"Status Values","text":"<ul> <li><code>executing</code>: Node execution has started</li> <li><code>completed</code>: Node execution finished successfully</li> <li><code>error</code>: Node execution failed with an error</li> </ul>"},{"location":"frontend/types/#example-executing-status","title":"Example - Executing Status","text":"<pre><code>const executingStatus: ExecutionStatus = {\n  node_id: \"node_1\",\n  node_number: 1,\n  node_name: \"add\",\n  status: \"executing\",\n  inputs: {\n    a: \"5\",\n    b: \"10\"\n  }\n};\n</code></pre>"},{"location":"frontend/types/#example-completed-status","title":"Example - Completed Status","text":"<pre><code>const completedStatus: ExecutionStatus = {\n  node_id: \"node_1\",\n  node_number: 1,\n  node_name: \"add\",\n  status: \"completed\",\n  inputs: {\n    a: \"5\",\n    b: \"10\"\n  },\n  output: 15\n};\n</code></pre>"},{"location":"frontend/types/#example-error-status","title":"Example - Error Status","text":"<pre><code>const errorStatus: ExecutionStatus = {\n  node_id: \"node_2\",\n  node_number: 2,\n  node_name: \"divide\",\n  status: \"error\",\n  inputs: {\n    a: \"10\",\n    b: \"0\"\n  },\n  error: \"division by zero\"\n};\n</code></pre>"},{"location":"frontend/types/#usage-context","title":"Usage Context","text":"<p>ExecutionStatus objects are received via Server-Sent Events during graph execution:</p> <pre><code>api.executeGraphStreaming(\n  request,\n  (status: ExecutionStatus) =&gt; {\n    // Handle status update\n    console.log(`${status.node_name}: ${status.status}`);\n  },\n  onComplete,\n  onError\n);\n</code></pre>"},{"location":"frontend/types/#type-usage-guidelines","title":"Type Usage Guidelines","text":""},{"location":"frontend/types/#importing-types","title":"Importing Types","text":"<pre><code>import type { \n  FunctionSchema, \n  NodeData, \n  ExecuteRequest, \n  ExecuteResponse \n} from '../types/schema';\n</code></pre>"},{"location":"frontend/types/#type-guards","title":"Type Guards","text":"<p>When working with dynamic <code>NodeData</code>, consider creating type guards:</p> <pre><code>function isFunctionNodeData(data: NodeData): data is NodeData &amp; {\n  functionName: string;\n  params: ParamSchema[];\n} {\n  return 'functionName' in data &amp;&amp; 'params' in data;\n}\n</code></pre>"},{"location":"frontend/types/#extending-types","title":"Extending Types","text":"<p>For component-specific needs, extend the base types:</p> <pre><code>interface FunctionNodeData extends NodeData {\n  functionName: string;\n  params: ParamSchema[];\n  onChange: (nodeId: string, param: string, value: string) =&gt; void;\n}\n</code></pre>"},{"location":"frontend/utils/","title":"Utilities","text":""},{"location":"frontend/utils/#api-client","title":"API Client","text":"<p>The API client module provides functions for communicating with the Psynapse backend server.</p>"},{"location":"frontend/utils/#configuration","title":"Configuration","text":"<pre><code>const API_BASE_URL = 'http://localhost:8000';\n</code></pre> <p>The default base URL points to a local backend server running on port 8000.</p>"},{"location":"frontend/utils/#api-methods","title":"API Methods","text":""},{"location":"frontend/utils/#getschemas","title":"getSchemas","text":"<p>Fetches available function schemas from the backend.</p>"},{"location":"frontend/utils/#signature","title":"Signature","text":"<pre><code>async getSchemas(): Promise&lt;FunctionSchema[]&gt;\n</code></pre>"},{"location":"frontend/utils/#returns","title":"Returns","text":"<p>A promise that resolves to an array of <code>FunctionSchema</code> objects.</p>"},{"location":"frontend/utils/#endpoint","title":"Endpoint","text":"<pre><code>GET /get_schema\n</code></pre>"},{"location":"frontend/utils/#response-format","title":"Response Format","text":"<pre><code>[\n  {\n    name: string;\n    params: ParamSchema[];\n    returns: ReturnSchema[];\n    docstring: string;\n    filepath: string;\n  },\n  ...\n]\n</code></pre>"},{"location":"frontend/utils/#usage-example","title":"Usage Example","text":"<pre><code>import { api } from '../utils/api';\n\nasync function loadSchemas() {\n  try {\n    const schemas = await api.getSchemas();\n    console.log('Available functions:', schemas);\n  } catch (error) {\n    console.error('Failed to load schemas:', error);\n  }\n}\n</code></pre>"},{"location":"frontend/utils/#error-handling","title":"Error Handling","text":"<ul> <li>Throws an error if the request fails</li> <li>Network errors are propagated from axios</li> <li>HTTP error responses (4xx, 5xx) throw exceptions</li> </ul>"},{"location":"frontend/utils/#executegraph","title":"executeGraph","text":"<p>Executes a computational graph on the backend.</p>"},{"location":"frontend/utils/#signature_1","title":"Signature","text":"<pre><code>async executeGraph(request: ExecuteRequest): Promise&lt;ExecuteResponse&gt;\n</code></pre>"},{"location":"frontend/utils/#parameters","title":"Parameters","text":"Parameter Type Description <code>request</code> <code>ExecuteRequest</code> The graph execution request containing nodes and edges"},{"location":"frontend/utils/#executerequest-structure","title":"ExecuteRequest Structure","text":"<pre><code>{\n  nodes: Array&lt;{\n    id: string;\n    type: string;\n    data: any;\n  }&gt;;\n  edges: Array&lt;{\n    source: string;\n    target: string;\n    sourceHandle: string;\n    targetHandle: string;\n  }&gt;;\n}\n</code></pre>"},{"location":"frontend/utils/#returns_1","title":"Returns","text":"<p>A promise that resolves to an <code>ExecuteResponse</code> object.</p>"},{"location":"frontend/utils/#executeresponse-structure","title":"ExecuteResponse Structure","text":"<pre><code>{\n  results: {\n    [nodeId: string]: any;\n  }\n}\n</code></pre> <p>The <code>results</code> object maps node IDs to their computed values. Typically includes results for ViewNodes and intermediate computations.</p>"},{"location":"frontend/utils/#endpoint_1","title":"Endpoint","text":"<pre><code>POST /execute\n</code></pre>"},{"location":"frontend/utils/#usage-example_1","title":"Usage Example","text":"<pre><code>import { api } from '../utils/api';\n\nasync function runGraph(nodes, edges) {\n  try {\n    const response = await api.executeGraph({\n      nodes: nodes.map(n =&gt; ({\n        id: n.id,\n        type: n.type,\n        data: n.data,\n      })),\n      edges: edges.map(e =&gt; ({\n        source: e.source,\n        target: e.target,\n        sourceHandle: e.sourceHandle,\n        targetHandle: e.targetHandle,\n      })),\n    });\n\n    console.log('Execution results:', response.results);\n    return response.results;\n  } catch (error) {\n    console.error('Graph execution failed:', error);\n    throw error;\n  }\n}\n</code></pre>"},{"location":"frontend/utils/#error-handling_1","title":"Error Handling","text":"<ul> <li>Throws an error if the request fails</li> <li>Network errors are propagated from axios</li> <li>Backend validation errors are returned in the response</li> <li>HTTP error responses include error details in the exception</li> </ul>"},{"location":"frontend/utils/#executegraphstreaming","title":"executeGraphStreaming","text":"<p>Executes a computational graph with real-time status updates via Server-Sent Events (SSE).</p>"},{"location":"frontend/utils/#signature_2","title":"Signature","text":"<pre><code>executeGraphStreaming(\n  request: ExecuteRequest,\n  onStatus: (status: ExecutionStatus) =&gt; void,\n  onComplete: (results: { [nodeId: string]: any }) =&gt; void,\n  onError: (error: string) =&gt; void\n): () =&gt; void\n</code></pre>"},{"location":"frontend/utils/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>request</code> <code>ExecuteRequest</code> The graph execution request containing nodes and edges <code>onStatus</code> <code>(status: ExecutionStatus) =&gt; void</code> Callback invoked for each node execution status update <code>onComplete</code> <code>(results: { [nodeId: string]: any }) =&gt; void</code> Callback invoked when execution completes successfully <code>onError</code> <code>(error: string) =&gt; void</code> Callback invoked if execution fails"},{"location":"frontend/utils/#returns_2","title":"Returns","text":"<p>A cleanup function that aborts the ongoing execution when called.</p>"},{"location":"frontend/utils/#executionstatus-structure","title":"ExecutionStatus Structure","text":"<pre><code>{\n  node_id: string;\n  node_number: number;\n  node_name: string;\n  status: 'executing' | 'completed' | 'error';\n  inputs?: Record&lt;string, any&gt;;\n  output?: any;\n  error?: string;\n}\n</code></pre>"},{"location":"frontend/utils/#endpoint_2","title":"Endpoint","text":"<pre><code>POST /execute/stream\n</code></pre>"},{"location":"frontend/utils/#sse-event-format","title":"SSE Event Format","text":"<p>The backend streams events in Server-Sent Events format:</p> <pre><code>data: {\"node_id\":\"node_1\",\"node_number\":1,\"node_name\":\"add\",\"status\":\"executing\",\"inputs\":{\"a\":\"5\",\"b\":\"10\"}}\n\ndata: {\"node_id\":\"node_1\",\"node_number\":1,\"node_name\":\"add\",\"status\":\"completed\",\"inputs\":{\"a\":\"5\",\"b\":\"10\"},\"output\":15}\n\ndata: {\"status\":\"done\",\"results\":{\"view_node_id\":15}}\n</code></pre>"},{"location":"frontend/utils/#usage-example_2","title":"Usage Example","text":"<pre><code>import { api } from '../utils/api';\n\nfunction executeWithStatus(nodes, edges) {\n  const cleanup = api.executeGraphStreaming(\n    {\n      nodes: nodes.map(n =&gt; ({ id: n.id, type: n.type, data: n.data })),\n      edges: edges.map(e =&gt; ({\n        source: e.source,\n        target: e.target,\n        sourceHandle: e.sourceHandle,\n        targetHandle: e.targetHandle,\n      })),\n    },\n    // onStatus callback\n    (status) =&gt; {\n      console.log(`Node ${status.node_number}: ${status.node_name} - ${status.status}`);\n      if (status.status === 'completed') {\n        console.log('Output:', status.output);\n      }\n    },\n    // onComplete callback\n    (results) =&gt; {\n      console.log('Execution complete:', results);\n    },\n    // onError callback\n    (error) =&gt; {\n      console.error('Execution failed:', error);\n    }\n  );\n\n  // To abort execution:\n  // cleanup();\n\n  return cleanup;\n}\n</code></pre>"},{"location":"frontend/utils/#execution-flow","title":"Execution Flow","text":"<ol> <li>Connection: Establishes SSE connection using fetch API with POST method</li> <li>Streaming: Receives status updates as nodes execute in topological order</li> <li>Status Updates: </li> <li>\"executing\" status when node starts</li> <li>\"completed\" status with output when node finishes</li> <li>\"error\" status with error message if node fails</li> <li>Completion: Receives \"done\" event with final ViewNode results</li> <li>Cleanup: Connection automatically closes, or can be aborted via cleanup function</li> </ol>"},{"location":"frontend/utils/#error-handling_2","title":"Error Handling","text":"<ul> <li>Network errors are caught and passed to <code>onError</code> callback</li> <li>Backend execution errors are streamed as error events</li> <li>Connection can be aborted using the returned cleanup function</li> <li>Abort errors are silently ignored (not passed to <code>onError</code>)</li> </ul>"},{"location":"frontend/utils/#implementation-details","title":"Implementation Details","text":"<ul> <li>Uses fetch API with <code>ReadableStream</code> for SSE support</li> <li>Supports POST requests (unlike EventSource API)</li> <li>Parses <code>data:</code> prefixed SSE messages</li> <li>Handles incomplete messages with buffer management</li> <li>Provides AbortController for cancellation</li> </ul>"},{"location":"frontend/utils/#http-client","title":"HTTP Client","text":"<p>The API client uses axios for HTTP requests (for non-streaming endpoints) and the native fetch API for Server-Sent Events, providing:</p> <ul> <li>Automatic JSON serialization/deserialization</li> <li>Promise-based API</li> <li>Request/response interceptors (if needed)</li> <li>Timeout handling</li> <li>Error handling</li> </ul>"},{"location":"frontend/utils/#type-safety","title":"Type Safety","text":"<p>All API methods are fully typed using TypeScript interfaces from <code>../types/schema</code>, ensuring type safety throughout the application.</p>"},{"location":"frontend/utils/#extensibility","title":"Extensibility","text":"<p>To add new API methods:</p> <ol> <li>Define request/response types in <code>../types/schema.ts</code></li> <li>Add method to the <code>api</code> object</li> <li>Use axios for the HTTP request</li> <li>Return properly typed response</li> </ol> <p>Example:</p> <pre><code>export const api = {\n  // ... existing methods ...\n\n  async newMethod(param: ParamType): Promise&lt;ReturnType&gt; {\n    const response = await axios.post&lt;ReturnType&gt;(\n      `${API_BASE_URL}/new_endpoint`,\n      param\n    );\n    return response.data;\n  },\n};\n</code></pre>"},{"location":"guides/annotated-dicts/","title":"AnnotatedDict: Multiple Output Handles","text":"<p><code>AnnotatedDict</code> is a special return type annotation that allows nodepack functions to have multiple output handles. Instead of a single \"output\" socket, nodes can expose individual dictionary keys as separate output connections.</p>"},{"location":"guides/annotated-dicts/#basic-usage","title":"Basic Usage","text":"<p>Import <code>AnnotatedDict</code> from the schema extractor and use it with <code>Literal</code> to specify output keys:</p> <pre><code>from typing import Literal\nfrom psynapse_backend.schema_extractor import AnnotatedDict\n\ndef split_name(full_name: str) -&gt; AnnotatedDict[Literal[\"first\", \"last\"]]:\n    \"\"\"Split a full name into first and last name.\"\"\"\n    parts = full_name.strip().split(\" \", 1)\n    return {\n        \"first\": parts[0],\n        \"last\": parts[1] if len(parts) &gt; 1 else \"\",\n    }\n</code></pre> <p>This creates a node with two output handles: <code>first</code> and <code>last</code>. Downstream nodes can connect to either output independently.</p> <p></p>"},{"location":"guides/annotated-dicts/#how-it-works","title":"How It Works","text":"<ol> <li>Schema Extraction: When the backend loads your nodepack, it detects the <code>AnnotatedDict[Literal[...]]</code> return type and extracts the key names</li> <li>Frontend Rendering: The node displays multiple output handles on the right side, one for each key</li> <li>Execution: When the node executes, it returns a dictionary. The executor routes each key's value to the appropriate connected edges</li> </ol>"},{"location":"guides/annotated-dicts/#example-mathematical-operations","title":"Example: Mathematical Operations","text":"<pre><code>def divmod_numbers(a: float, b: float) -&gt; AnnotatedDict[Literal[\"quotient\", \"remainder\"]]:\n    \"\"\"Calculate both quotient and remainder of a division.\"\"\"\n    if b == 0:\n        raise ValueError(\"Division by zero\")\n    return {\n        \"quotient\": a // b,\n        \"remainder\": a % b,\n    }\n</code></pre>"},{"location":"guides/annotated-dicts/#validation","title":"Validation","text":"<p>The executor validates that the returned dictionary contains all expected keys. If a key is missing, a <code>ValueError</code> is raised:</p> <pre><code>ValueError: AnnotatedDict output missing expected key 'first'. Available keys: ['name']\n</code></pre>"},{"location":"guides/annotated-dicts/#connecting-to-downstream-nodes","title":"Connecting to Downstream Nodes","text":"<p>When connecting an <code>AnnotatedDict</code> node to another node:</p> <ul> <li>Each output handle corresponds to a specific dictionary key</li> <li>The connected downstream node receives only the value for that key (not the full dictionary)</li> <li>Multiple downstream nodes can connect to different outputs from the same source node</li> </ul>"},{"location":"guides/annotated-dicts/#comparison-with-regular-dict","title":"Comparison with Regular Dict","text":"Feature <code>dict</code> return <code>AnnotatedDict</code> return Output handles Single \"output\" One per specified key Value passed Full dictionary Individual key values Schema <code>returns: [{name: \"result\", type: \"dict\"}]</code> <code>returns: [{name: \"key1\"}, {name: \"key2\"}, ...]</code>"},{"location":"guides/annotated-dicts/#best-practices","title":"Best Practices","text":"<ol> <li>Use descriptive key names: Keys become handle labels in the UI</li> <li>Keep key count reasonable: Too many outputs can clutter the node</li> <li>Document the keys: Include key descriptions in your docstring</li> <li>Always return all keys: The executor validates that all declared keys are present</li> </ol>"},{"location":"guides/annotated-dicts/#limitations","title":"Limitations","text":"<ul> <li>Only top-level dictionary keys are supported (no nested access like <code>user.name</code>)</li> <li>Keys must be string literals defined at annotation time</li> <li>The function must return a plain dictionary (not a custom dict subclass)</li> </ul>"},{"location":"guides/architecture/","title":"Psynapse Architecture Documentation","text":""},{"location":"guides/architecture/#overview","title":"Overview","text":"<p>Psynapse is a full-stack node-based workflow editor that enables visual programming through a drag-and-drop interface. The system consists of three main components:</p> <ol> <li>Backend (FastAPI): Handles schema extraction from Python functions and executes node graphs</li> <li>Frontend (React + React Flow): Provides visual node graph editor with drag-and-drop functionality</li> <li>Nodepacks: Python modules containing executable functions</li> </ol>"},{"location":"guides/architecture/#system-architecture","title":"System Architecture","text":"graph TB     subgraph Frontend[\"Frontend (React + Vite)\"]         NodeLib[Node Library Panel]         Canvas[ReactFlow Canvas]         ExecBtn[Execute Button]         Status[Status Panel&lt;br/&gt;Real-time Updates]     end      subgraph Backend[\"Backend (FastAPI)\"]         SchemaEP[\"GET /get_schema&lt;br/&gt;Endpoint\"]         ExecEP[\"POST /execute&lt;br/&gt;Endpoint\"]         StreamEP[\"POST /execute/stream&lt;br/&gt;Endpoint SSE\"]          SchemaExt[Schema Extractor&lt;br/&gt;- inspect module&lt;br/&gt;- type hints&lt;br/&gt;- docstrings]         GraphExec[Graph Executor&lt;br/&gt;- Topological Sort&lt;br/&gt;- Sequential Execution&lt;br/&gt;- Type Conversion]         StreamExec[Streaming Executor&lt;br/&gt;- SSE Events&lt;br/&gt;- Progress Updates&lt;br/&gt;- Real-time Status]          FuncReg[Function Registry]         ProgReg[Progress Class Registry]         StreamReg[Stream Class Registry]     end      subgraph Nodepacks[\"Nodepacks\"]         OpsFile[ops.py&lt;br/&gt;Regular Functions]         ProgFile[progress_ops.py&lt;br/&gt;Progress Classes]         StreamFile[stream_ops.py&lt;br/&gt;Stream Classes]     end      NodeLib --&gt;|\"HTTP GET\"| SchemaEP     SchemaEP --&gt; SchemaExt     SchemaExt --&gt;|\"Introspect\"| OpsFile     SchemaExt --&gt;|\"Introspect\"| ProgFile     SchemaExt --&gt;|\"Introspect\"| StreamFile      Canvas --&gt; ExecBtn     ExecBtn --&gt;|\"HTTP POST\"| ExecEP     ExecBtn --&gt;|\"HTTP POST + SSE\"| StreamEP      ExecEP --&gt; GraphExec     StreamEP --&gt; StreamExec      GraphExec --&gt;|\"Load\"| FuncReg     GraphExec --&gt;|\"Load\"| ProgReg     GraphExec --&gt;|\"Load\"| StreamReg     StreamExec --&gt;|\"Load\"| FuncReg     StreamExec --&gt;|\"Load\"| ProgReg     StreamExec --&gt;|\"Load\"| StreamReg      FuncReg -.-&gt;|\"Import\"| OpsFile     ProgReg -.-&gt;|\"Import\"| ProgFile     StreamReg -.-&gt;|\"Import\"| StreamFile      StreamEP --&gt;|\"SSE Stream\"| Status     ExecEP --&gt;|\"JSON Response\"| Canvas      style Frontend fill:#e1f5ff     style Backend fill:#fff4e1     style Nodepacks fill:#e8f5e9  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"guides/architecture/#backend-architecture","title":"Backend Architecture","text":""},{"location":"guides/architecture/#1-schema-extraction-schema_extractorpy","title":"1. Schema Extraction (<code>schema_extractor.py</code>)","text":"<p>Purpose: Dynamically discover and extract metadata from Python functions and classes in nodepacks.</p> <p>Process: 1. Scan <code>nodepacks/</code> directory for all <code>ops.py</code>, <code>progress_ops.py</code>, and <code>stream_ops.py</code> files 2. Load each module dynamically using <code>importlib</code> 3. For each function in <code>ops.py</code>:    - Extract function signature using <code>inspect.signature()</code>    - Parse type hints using <code>typing.get_type_hints()</code>    - Extract docstrings using <code>inspect.getdoc()</code>    - Detect <code>Literal</code> type hints for dropdown UI controls    - Build JSON schema with parameters, defaults, and return types 4. For each class in <code>progress_ops.py</code>:    - Extract <code>__call__</code> method signature (skip private classes starting with <code>_</code>)    - Parse type hints from <code>__call__</code> method (excluding <code>self</code> parameter)    - Extract docstrings from class or <code>__call__</code> method    - Detect <code>Literal</code> type hints for dropdown UI controls    - Build JSON schema with <code>is_progress_node: true</code> flag 5. For each class in <code>stream_ops.py</code>:    - Extract <code>__call__</code> method signature (skip private classes starting with <code>_</code>)    - Parse type hints from <code>__call__</code> method (excluding <code>self</code> parameter)    - Extract docstrings from class or <code>__call__</code> method    - Detect <code>Literal</code> type hints for dropdown UI controls    - Build JSON schema with <code>is_stream_node: true</code> flag</p> <p>Output Schema: <pre><code>{\n  \"name\": \"function_name\",\n  \"params\": [\n    {\n      \"name\": \"param1\",\n      \"type\": \"float\"\n    },\n    {\n      \"name\": \"param2\",\n      \"type\": \"int\",\n      \"default\": 10\n    },\n    {\n      \"name\": \"mode\",\n      \"type\": \"Literal\",\n      \"literal_values\": [\"fast\", \"slow\", \"medium\"]\n    }\n  ],\n  \"returns\": [\n    {\"name\": \"result\", \"type\": \"float\"}\n  ],\n  \"docstring\": \"Function description...\",\n  \"filepath\": \"/path/to/ops.py\",\n  \"is_progress_node\": false\n}\n</code></pre></p>"},{"location":"guides/architecture/#2-graph-executor-executorpy","title":"2. Graph Executor (<code>executor.py</code>)","text":"<p>Purpose: Execute node graphs in the correct order with dependency resolution.</p> <p>Key Components: - Function Registry: Dictionary mapping function names to callable functions from <code>ops.py</code> - Progress Class Registry: Dictionary mapping class names to classes from <code>progress_ops.py</code> - Stream Class Registry: Dictionary mapping class names to classes from <code>stream_ops.py</code></p>"},{"location":"guides/architecture/#topological-sorting-kahns-algorithm","title":"Topological Sorting (Kahn's Algorithm)","text":"<pre><code>1. Calculate in-degree for each node (count of incoming edges)\n2. Initialize queue with nodes having in-degree = 0\n3. While queue is not empty:\n   a. Dequeue node and add to sorted list\n   b. For each neighbor, decrease in-degree by 1\n   c. If neighbor's in-degree becomes 0, enqueue it\n4. If sorted list size \u2260 total nodes, graph has cycle (raises ValueError)\n</code></pre>"},{"location":"guides/architecture/#node-execution-flow","title":"Node Execution Flow","text":"<p>The executor supports four node types, each with distinct behavior:</p> <ol> <li>FunctionNode (from <code>ops.py</code>):</li> <li>Gather inputs: First use node data (default values), then override with connected edge values</li> <li>Type conversion: Automatically convert string inputs to appropriate types (int, float, str, bool)</li> <li>Function call: Execute corresponding Python function with converted inputs</li> <li> <p>Store output for downstream nodes</p> </li> <li> <p>VariableNode:</p> </li> <li>Output stored value based on <code>variableType</code>: String, Number, Boolean, List, Object, Image</li> <li>Support for LLM message formatting:<ul> <li><code>llmMessageFormat</code>: Wraps output as <code>{\"role\": \"&lt;role&gt;\", \"content\": \"&lt;value&gt;\"}</code></li> <li><code>textContentFormat</code>: Wraps as <code>{\"type\": \"text\", \"content\": \"&lt;value&gt;\"}</code> (legacy)</li> <li><code>imageContentFormat</code>: For image data URLs with optional LLM message format</li> </ul> </li> <li> <p>Type conversion: Parse strings to numbers/booleans, JSON strings to objects, etc.</p> </li> <li> <p>ListNode:</p> </li> <li>Aggregates values from multiple connected inputs into a single list</li> <li>Input order preserved via indexed handles (<code>input-0</code>, <code>input-1</code>, etc.)</li> <li> <p>Outputs Python list of collected values</p> </li> <li> <p>ViewNode:</p> </li> <li>Receive single input from connected edge</li> <li>Store value for display (no processing)</li> <li> <p>Results returned in API response</p> </li> <li> <p>Progress Nodes (from <code>progress_ops.py</code>):</p> </li> <li>Instantiate progress-aware class</li> <li>Set up <code>_progress_reporter</code> with callback mechanism</li> <li>Execute <code>__call__</code> method in separate thread</li> <li>Stream progress updates via callback during execution</li> <li>Progress format: <code>{percent: 0.0-1.0, message: \"...\"}</code></li> <li> <p>Store output when complete</p> </li> <li> <p>Stream Nodes (from <code>stream_ops.py</code>):</p> </li> <li>Instantiate stream-aware class</li> <li>Set up <code>_stream_reporter</code> with callback mechanism</li> <li>Execute <code>__call__</code> method in separate thread</li> <li>Stream text chunks via callback during execution (e.g., LLM token streaming)</li> <li>Streaming format: <code>{streaming_text: \"accumulated...\", streaming_chunk: \"new chunk\"}</code></li> <li>Store final output when complete</li> </ol>"},{"location":"guides/architecture/#error-handling","title":"Error Handling","text":"<ul> <li>Type conversion errors (caught and logged)</li> <li>Missing functions/classes in registries</li> <li>Function execution exceptions (output set to None)</li> <li>Thread execution errors for progress and stream nodes</li> <li>Cycle detection in graph (raises ValueError)</li> </ul>"},{"location":"guides/architecture/#environment-variables","title":"Environment Variables","text":"<ul> <li>Accepts optional <code>env_vars</code> parameter</li> <li>Temporarily sets environment variables during execution</li> <li>Restores original values after completion</li> </ul>"},{"location":"guides/architecture/#3-api-endpoints-mainpy","title":"3. API Endpoints (<code>main.py</code>)","text":"<p>Initialization: - Uses FastAPI lifespan events to initialize GraphExecutor on startup - Loads nodepacks from <code>NODEPACKS_DIR</code> environment variable (default: <code>./nodepacks</code>) - Populates function, progress class, and stream class registries on startup</p> <p>Endpoints:</p>"},{"location":"guides/architecture/#get","title":"GET <code>/</code>","text":"<ul> <li>Health check endpoint</li> <li>Returns: <code>{\"message\": \"Psynapse Backend API\"}</code></li> </ul>"},{"location":"guides/architecture/#get-get_schema","title":"GET <code>/get_schema</code>","text":"<ul> <li>Extracts and returns schemas for all functions/classes in nodepacks</li> <li>Calls <code>extract_all_schemas()</code> to scan all <code>ops.py</code>, <code>progress_ops.py</code>, and <code>stream_ops.py</code> files</li> <li>Used by frontend on startup to populate node library</li> <li>Response: Array of schema objects (see Schema Extraction section)</li> </ul>"},{"location":"guides/architecture/#post-execute","title":"POST <code>/execute</code>","text":"<ul> <li>Accepts: <code>ExecuteRequest</code> with <code>nodes</code>, <code>edges</code>, and optional <code>env_vars</code></li> <li>Executes graph using <code>GraphExecutor.execute_graph()</code></li> <li>Returns: ViewNode results only</li> <li>Response format: <code>{\"results\": {\"view_node_id\": value}}</code></li> <li>Traditional single-response execution (no streaming)</li> </ul>"},{"location":"guides/architecture/#post-executestream","title":"POST <code>/execute/stream</code>","text":"<ul> <li>Accepts: <code>ExecuteRequest</code> with <code>nodes</code>, <code>edges</code>, and optional <code>env_vars</code></li> <li>Streams execution status via Server-Sent Events (SSE)</li> <li>Provides real-time updates as nodes execute</li> <li>Response headers include <code>Cache-Control: no-cache</code> and <code>X-Accel-Buffering: no</code></li> <li>Returns status for each node: <code>executing</code> \u2192 <code>completed</code>/<code>error</code></li> <li>Progress nodes also emit <code>progress</code> status during execution</li> <li>Stream nodes emit <code>streaming</code> status with text chunks during execution</li> <li>Final event contains ViewNode results</li> </ul> <p>SSE Event Format: <pre><code>data: {\"node_id\":\"node_1\",\"node_number\":1,\"node_name\":\"add\",\"status\":\"executing\",\"inputs\":{\"a\":\"5\",\"b\":\"10\"}}\n\ndata: {\"node_id\":\"node_1\",\"node_number\":1,\"node_name\":\"add\",\"status\":\"completed\",\"inputs\":{\"a\":\"5\",\"b\":\"10\"},\"output\":15}\n\ndata: {\"status\":\"done\",\"results\":{\"view_node_id\":15}}\n</code></pre></p> <p>Progress Node Event Format: <pre><code>data: {\"node_id\":\"node_2\",\"node_number\":2,\"node_name\":\"ProgressOp\",\"status\":\"executing\",\"inputs\":{\"count\":10}}\n\ndata: {\"node_id\":\"node_2\",\"node_number\":2,\"node_name\":\"ProgressOp\",\"status\":\"progress\",\"progress\":0.3,\"progress_message\":\"Processing item 3/10\",\"inputs\":{\"count\":10}}\n\ndata: {\"node_id\":\"node_2\",\"node_number\":2,\"node_name\":\"ProgressOp\",\"status\":\"progress\",\"progress\":0.6,\"progress_message\":\"Processing item 6/10\",\"inputs\":{\"count\":10}}\n\ndata: {\"node_id\":\"node_2\",\"node_number\":2,\"node_name\":\"ProgressOp\",\"status\":\"completed\",\"inputs\":{\"count\":10},\"output\":90}\n</code></pre></p> <p>Stream Node Event Format: <pre><code>data: {\"node_id\":\"node_3\",\"node_number\":3,\"node_name\":\"OpenAIChatCompletionStream\",\"status\":\"executing\",\"inputs\":{\"model\":\"gpt-4\",\"messages\":[...]}}\n\ndata: {\"node_id\":\"node_3\",\"node_number\":3,\"node_name\":\"OpenAIChatCompletionStream\",\"status\":\"streaming\",\"streaming_text\":\"Hello\",\"streaming_chunk\":\"Hello\",\"inputs\":{\"model\":\"gpt-4\",\"messages\":[...]}}\n\ndata: {\"node_id\":\"node_3\",\"node_number\":3,\"node_name\":\"OpenAIChatCompletionStream\",\"status\":\"streaming\",\"streaming_text\":\"Hello, world\",\"streaming_chunk\":\", world\",\"inputs\":{\"model\":\"gpt-4\",\"messages\":[...]}}\n\ndata: {\"node_id\":\"node_3\",\"node_number\":3,\"node_name\":\"OpenAIChatCompletionStream\",\"status\":\"completed\",\"inputs\":{\"model\":\"gpt-4\",\"messages\":[...]},\"output\":{\"id\":\"chatcmpl-...\",\"choices\":[...]}}\n</code></pre></p> <p>Error Event Format: <pre><code>data: {\"node_id\":\"node_3\",\"node_number\":3,\"node_name\":\"divide\",\"status\":\"error\",\"inputs\":{\"a\":10,\"b\":0},\"error\":\"division by zero\"}\n\ndata: {\"status\":\"error\",\"error\":\"Graph contains a cycle\"}\n</code></pre></p>"},{"location":"guides/architecture/#cli-commands","title":"CLI Commands","text":"<p>The backend includes a Typer CLI with the <code>run</code> command: <pre><code>psynapse-backend run --host 0.0.0.0 --port 8000 --reload --nodepack-dir ./nodepacks\n</code></pre></p>"},{"location":"guides/architecture/#cors-configuration","title":"CORS Configuration","text":"<ul> <li>Allows all origins (<code>*</code>) - should be restricted in production</li> <li>Allows credentials, all methods, and all headers</li> </ul>"},{"location":"guides/architecture/#frontend-architecture","title":"Frontend Architecture","text":""},{"location":"guides/architecture/#component-hierarchy","title":"Component Hierarchy","text":"graph TD     App[App]     App --&gt; Editor[PsynapseEditor&lt;br/&gt;ReactFlowProvider]      Editor --&gt; Library[NodeLibraryPanel]     Editor --&gt; Canvas[ReactFlow Canvas]     Editor --&gt; Status[StatusPanel]      Library --&gt; BuiltIn[Built-in Nodes]     Library --&gt; FuncNodes[Function Nodes&lt;br/&gt;Dynamic from schemas]      BuiltIn --&gt; VarNode[VariableNode]     BuiltIn --&gt; ListNodeLib[ListNode]     BuiltIn --&gt; ViewNodeLib[ViewNode]      Canvas --&gt; Controls[Controls]     Canvas --&gt; MiniMap[MiniMap]     Canvas --&gt; Background[Background]     Canvas --&gt; CustomNodes[Custom Node Components]      CustomNodes --&gt; FuncNodeComp[FunctionNode]     CustomNodes --&gt; VarNodeComp[VariableNode]     CustomNodes --&gt; ListNodeComp[ListNode]     CustomNodes --&gt; ViewNodeComp[ViewNode]      Status --&gt; ExecStatus[ExecutionStatus Array]     ExecStatus --&gt; Badge[Node Number Badge]     ExecStatus --&gt; Name[Node Name]     ExecStatus --&gt; StatusIcon[Status Spinner/Icon]     ExecStatus --&gt; Progress[Progress Bar&lt;br/&gt;for Progress Nodes]     ExecStatus --&gt; Inputs[Collapsible Inputs]     ExecStatus --&gt; Outputs[Outputs/Errors]      style App fill:#e1f5ff     style Editor fill:#b3e5fc     style Library fill:#fff9c4     style Canvas fill:#c8e6c9     style Status fill:#ffccbc  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"guides/architecture/#key-components","title":"Key Components","text":""},{"location":"guides/architecture/#1-psynapseeditor-psynapseeditortsx","title":"1. PsynapseEditor (<code>PsynapseEditor.tsx</code>)","text":"<p>Responsibilities: - Manage React Flow state (nodes, edges) - Handle drag-and-drop from library panel - Coordinate graph execution via streaming SSE - Update ViewNodes with results - Manage execution status and history</p> <p>State Management: <pre><code>- nodes: Node[]                    // All nodes on canvas\n- edges: Edge[]                    // All connections\n- reactFlowInstance                // React Flow API\n- executing: boolean               // Execution status flag\n- statusHistory: ExecutionStatus[] // Real-time execution updates\n- abortExecution: (() =&gt; void)     // Cleanup function for aborting SSE stream\n</code></pre></p> <p>Key Functions: - <code>onDrop</code>: Create new node from dragged library item (FunctionNode, VariableNode, ListNode, ViewNode) - <code>executeGraph</code>: Serialize graph and stream execution from backend via SSE - <code>handleNodeDataChange</code>: Update node input values and properties - Node type registry: Maps node types to their React components</p> <p>Execution Flow: 1. User clicks \"Execute\" button 2. Clears previous status history 3. Calls <code>api.executeGraphStreaming()</code> with status update callback 4. Receives real-time status updates via SSE 5. Updates <code>statusHistory</code> state as nodes execute 6. Shows progress bars for progress nodes 7. Updates ViewNodes with final results 8. Handles errors and displays them in StatusPanel</p>"},{"location":"guides/architecture/#2-nodelibrarypanel-nodelibrarypaneltsx","title":"2. NodeLibraryPanel (<code>NodeLibraryPanel.tsx</code>)","text":"<p>Responsibilities: - Fetch schemas from backend on mount using <code>useSchema</code> hook - Display built-in nodes (VariableNode, ListNode, ViewNode) - Display draggable function nodes from schemas - Handle drag events for all node types</p> <p>Built-in Nodes: - Variable: Stores values (String, Number, Boolean, List, Object, Image) - List: Aggregates multiple inputs into array - View: Displays output values</p> <p>Drag Data Format: <pre><code>{\n  type: 'functionNode' | 'variableNode' | 'listNode' | 'viewNode',\n  schema?: FunctionSchema  // For function nodes\n}\n</code></pre></p>"},{"location":"guides/architecture/#3-functionnode-functionnodetsx","title":"3. FunctionNode (<code>FunctionNode.tsx</code>)","text":"<p>Features: - Input handles for each parameter (only if no default value) - Input fields with type-appropriate controls:   - Text inputs for strings   - Number inputs for int/float   - Checkboxes for booleans   - Dropdowns for Literal types - Single output handle - Real-time value updates via node data changes</p> <p>Styling: - Blue border and handles - White background - Parameters without defaults show as input handles - Parameters with defaults show only in properties panel</p>"},{"location":"guides/architecture/#4-variablenode-variablenodetsx","title":"4. VariableNode (<code>VariableNode.tsx</code>)","text":"<p>Features: - No input handles (self-contained value storage) - Single output handle - Type selector: String, Number, Boolean, List, Object, Image - Value input appropriate to selected type - LLM message formatting options:   - <code>llmMessageFormat</code>: Wraps as <code>{\"role\": \"&lt;role&gt;\", \"content\": \"&lt;value&gt;\"}</code>   - Role selector for LLM messages (user, assistant, system) - Image upload support with data URL storage</p> <p>Styling: - Purple border and handles - White background - Type-specific input controls</p>"},{"location":"guides/architecture/#5-listnode-listnodetsx","title":"5. ListNode (<code>ListNode.tsx</code>)","text":"<p>Features: - Multiple indexed input handles (<code>input-0</code>, <code>input-1</code>, etc.) - Single output handle - Dynamically adds input handles as needed - Aggregates all connected inputs into array - Preserves input order</p> <p>Styling: - Orange border and handles - White background - Shows number of connected inputs</p>"},{"location":"guides/architecture/#6-viewnode-viewnodetsx","title":"6. ViewNode (<code>ViewNode.tsx</code>)","text":"<p>Features: - Single input handle - Display area for value (supports complex objects/arrays) - No output handles - Updates after execution with final results - JSON pretty-printing for complex values</p> <p>Styling: - Green border and handles - Light green background - Monospace font for values - Scrollable content area</p>"},{"location":"guides/architecture/#7-statuspanel-statuspaneltsx","title":"7. StatusPanel (<code>StatusPanel.tsx</code>)","text":"<p>Features: - Real-time execution monitoring via SSE stream - Displays nodes in execution order (numbered badges) - Status indicators:   - Animated spinner for <code>executing</code> nodes   - Progress bar for <code>progress</code> nodes (shows percentage and message)   - Streaming text display for <code>streaming</code> nodes (shows accumulated text in real-time)   - Green checkmark for <code>completed</code> nodes   - Red X for <code>error</code> nodes - Collapsible inputs section (click to expand/collapse) - Output/error display with JSON pretty-printing - Scrollable history - Auto-scrolls to latest status update</p> <p>Styling: - Fixed 400px width on right side - Color-coded status borders:   - Blue for executing   - Orange for progress   - Cyan for streaming   - Green for completed   - Red for error - White cards with colored left borders - Monospace font for input/output values</p> <p>State Updates: - Receives <code>ExecutionStatus[]</code> from parent PsynapseEditor - Updates in real-time as SSE events arrive - Shows execution progress visually with progress bars - Clears on new execution</p>"},{"location":"guides/architecture/#data-flow","title":"Data Flow","text":""},{"location":"guides/architecture/#1-initialization-flow","title":"1. Initialization Flow","text":"sequenceDiagram     participant User     participant Frontend     participant useSchema Hook     participant Backend     participant Nodepacks      User-&gt;&gt;Frontend: Opens application     Frontend-&gt;&gt;useSchema Hook: Mount component     useSchema Hook-&gt;&gt;Backend: GET /get_schema     Backend-&gt;&gt;Nodepacks: Scan ops.py, progress_ops.py, and stream_ops.py     Nodepacks--&gt;&gt;Backend: Function/Class metadata     Backend--&gt;&gt;useSchema Hook: Array of schemas     useSchema Hook--&gt;&gt;Frontend: Schemas available     Frontend-&gt;&gt;Frontend: Populate NodeLibraryPanel     User-&gt;&gt;Frontend: Drag nodes onto canvas  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"guides/architecture/#2-execution-flow-streaming-sse","title":"2. Execution Flow (Streaming SSE)","text":"sequenceDiagram     participant User     participant Frontend     participant Backend     participant GraphExecutor     participant Nodepacks      User-&gt;&gt;Frontend: Click \"Execute\" button     Frontend-&gt;&gt;Frontend: Clear previous status history     Frontend-&gt;&gt;Frontend: Serialize graph (nodes + edges)     Frontend-&gt;&gt;Backend: POST /execute/stream (SSE)     Backend-&gt;&gt;GraphExecutor: execute_graph_streaming()     GraphExecutor-&gt;&gt;GraphExecutor: Topological sort      loop For each node in order         GraphExecutor-&gt;&gt;Backend: Yield \"executing\" status         Backend--&gt;&gt;Frontend: SSE event: executing         Frontend-&gt;&gt;Frontend: Update StatusPanel (show spinner)          alt Function/Variable/List/View Node             GraphExecutor-&gt;&gt;Nodepacks: Call function or process node             Nodepacks--&gt;&gt;GraphExecutor: Return result             GraphExecutor-&gt;&gt;Backend: Yield \"completed\" status             Backend--&gt;&gt;Frontend: SSE event: completed         else Progress Node             GraphExecutor-&gt;&gt;Nodepacks: Call progress class __call__             loop Progress updates                 Nodepacks-&gt;&gt;GraphExecutor: Report progress (0.0-1.0)                 GraphExecutor-&gt;&gt;Backend: Yield \"progress\" status                 Backend--&gt;&gt;Frontend: SSE event: progress                 Frontend-&gt;&gt;Frontend: Update progress bar             end             Nodepacks--&gt;&gt;GraphExecutor: Return final result             GraphExecutor-&gt;&gt;Backend: Yield \"completed\" status             Backend--&gt;&gt;Frontend: SSE event: completed         else Stream Node             GraphExecutor-&gt;&gt;Nodepacks: Call stream class __call__             loop Streaming chunks                 Nodepacks-&gt;&gt;GraphExecutor: Emit text chunk                 GraphExecutor-&gt;&gt;Backend: Yield \"streaming\" status                 Backend--&gt;&gt;Frontend: SSE event: streaming                 Frontend-&gt;&gt;Frontend: Update streaming text display             end             Nodepacks--&gt;&gt;GraphExecutor: Return final result             GraphExecutor-&gt;&gt;Backend: Yield \"completed\" status             Backend--&gt;&gt;Frontend: SSE event: completed         end          Frontend-&gt;&gt;Frontend: Update StatusPanel (show result)     end      GraphExecutor-&gt;&gt;Backend: Yield \"done\" with ViewNode results     Backend--&gt;&gt;Frontend: SSE event: done     Frontend-&gt;&gt;Frontend: Update ViewNodes with results     Frontend-&gt;&gt;Frontend: Mark execution complete  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"guides/architecture/#3-node-type-data-flow","title":"3. Node Type Data Flow","text":"<p>FunctionNode: <pre><code>Input: Node data (defaults) + Connected edges (overrides)\n  \u2192 Type conversion (str \u2192 int/float/bool)\n  \u2192 Function execution\n  \u2192 Output stored in node_outputs dict\n</code></pre></p> <p>VariableNode: <pre><code>Input: variableValue + variableType + formatting options\n  \u2192 Type conversion based on variableType\n  \u2192 Apply LLM message formatting if enabled\n  \u2192 Output stored in node_outputs dict\n</code></pre></p> <p>ListNode: <pre><code>Input: Multiple connected edges with indexed handles (input-0, input-1, ...)\n  \u2192 Sort inputs by index\n  \u2192 Aggregate into Python list\n  \u2192 Output stored in node_outputs dict\n</code></pre></p> <p>ViewNode: <pre><code>Input: Single connected edge\n  \u2192 Pass through value\n  \u2192 Store in view_node_results dict\n  \u2192 Return in final API response\n</code></pre></p> <p>Progress Node: <pre><code>Input: Node data + Connected edges\n  \u2192 Instantiate progress class\n  \u2192 Set up _progress_reporter callback\n  \u2192 Execute in separate thread\n  \u2192 Yield progress updates (0.0-1.0 + message)\n  \u2192 Store final output in node_outputs dict\n</code></pre></p> <p>Stream Node: <pre><code>Input: Node data + Connected edges\n  \u2192 Instantiate stream class\n  \u2192 Set up _stream_reporter callback\n  \u2192 Execute in separate thread\n  \u2192 Yield streaming text chunks (accumulated + chunk)\n  \u2192 Store final output in node_outputs dict\n</code></pre></p>"},{"location":"guides/architecture/#node-graph-execution-details","title":"Node Graph Execution Details","text":""},{"location":"guides/architecture/#example-graph","title":"Example Graph","text":"graph LR     V1[VariableNode&lt;br/&gt;value: 5]     V2[VariableNode&lt;br/&gt;value: 3]     V3[VariableNode&lt;br/&gt;value: 2]     V4[VariableNode&lt;br/&gt;value: 4]      F1[FunctionNode&lt;br/&gt;add]     F2[FunctionNode&lt;br/&gt;add]     F3[FunctionNode&lt;br/&gt;multiply]      L1[ListNode]      View[ViewNode]      V1 --&gt;|output| F1     V2 --&gt;|output| F1     V3 --&gt;|output| F2     V4 --&gt;|output| F2      F1 --&gt;|output| F3     F2 --&gt;|output| F3      F1 --&gt;|output| L1     F2 --&gt;|output| L1      F3 --&gt;|output| View      style V1 fill:#e1bee7     style V2 fill:#e1bee7     style V3 fill:#e1bee7     style V4 fill:#e1bee7     style F1 fill:#bbdefb     style F2 fill:#bbdefb     style F3 fill:#bbdefb     style L1 fill:#ffe0b2     style View fill:#c8e6c9  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"guides/architecture/#execution-steps","title":"Execution Steps","text":"<ol> <li> <p>Topological Sort:    <pre><code>Order: V1 \u2192 V2 \u2192 V3 \u2192 V4 \u2192 F1(add) \u2192 F2(add) \u2192 L1(list) \u2192 F3(multiply) \u2192 View\n</code></pre></p> </li> <li> <p>Sequential Execution:    <pre><code>V1: output = 5\nV2: output = 3\nV3: output = 2\nV4: output = 4\nF1(add): inputs = {a: 5, b: 3} \u2192 result = 8\nF2(add): inputs = {a: 2, b: 4} \u2192 result = 6\nL1(list): inputs = [8, 6] \u2192 result = [8, 6]\nF3(multiply): inputs = {a: 8, b: 6} \u2192 result = 48\nView: input = 48 \u2192 display = 48\n</code></pre></p> </li> <li> <p>SSE Events Sequence:    <pre><code>// Variable nodes\n{\"node_id\":\"V1\",\"node_number\":1,\"node_name\":\"Variable\",\"status\":\"executing\",\"inputs\":{}}\n{\"node_id\":\"V1\",\"node_number\":1,\"node_name\":\"Variable\",\"status\":\"completed\",\"inputs\":{},\"output\":5}\n\n// ... (V2, V3, V4 similar)\n\n// Function nodes\n{\"node_id\":\"F1\",\"node_number\":5,\"node_name\":\"add\",\"status\":\"executing\",\"inputs\":{\"a\":5,\"b\":3}}\n{\"node_id\":\"F1\",\"node_number\":5,\"node_name\":\"add\",\"status\":\"completed\",\"inputs\":{\"a\":5,\"b\":3},\"output\":8}\n\n// List node\n{\"node_id\":\"L1\",\"node_number\":7,\"node_name\":\"List\",\"status\":\"executing\",\"inputs\":{\"input-0\":8,\"input-1\":6}}\n{\"node_id\":\"L1\",\"node_number\":7,\"node_name\":\"List\",\"status\":\"completed\",\"inputs\":{\"input-0\":8,\"input-1\":6},\"output\":[8,6]}\n\n// Final result\n{\"status\":\"done\",\"results\":{\"view_node_id\":\"View\",\"value\":48}}\n</code></pre></p> </li> <li> <p>Final Response:    <pre><code>{\n  \"results\": {\n    \"View\": 48\n  }\n}\n</code></pre></p> </li> </ol>"},{"location":"guides/architecture/#type-system","title":"Type System","text":""},{"location":"guides/architecture/#supported-types","title":"Supported Types","text":"<p>Backend and frontend support these types:</p>"},{"location":"guides/architecture/#primitive-types-for-function-parameters","title":"Primitive Types (for Function Parameters):","text":"<ul> <li><code>int</code>: Integers</li> <li><code>float</code>: Floating-point numbers</li> <li><code>str</code>: Strings</li> <li><code>bool</code>: Booleans</li> <li><code>Literal[...]</code>: Dropdown selection (e.g., <code>Literal[\"fast\", \"slow\"]</code>)</li> </ul>"},{"location":"guides/architecture/#variable-node-types","title":"Variable Node Types:","text":"<ul> <li>String: Text values</li> <li>Number: Integer or floating-point numbers</li> <li>Boolean: True/False values</li> <li>List: JSON array (e.g., <code>[1, 2, 3]</code>)</li> <li>Object: JSON object (e.g., <code>{\"key\": \"value\"}</code>)</li> <li>Image: Data URL for images (e.g., <code>data:image/png;base64,...</code>)</li> </ul>"},{"location":"guides/architecture/#complex-types-via-python-functions","title":"Complex Types (via Python functions):","text":"<ul> <li>dict: Python dictionaries (from Object variables or function returns)</li> <li>list: Python lists (from List nodes, List variables, or function returns)</li> <li>Any: Any Python type (used when type hints are missing)</li> </ul>"},{"location":"guides/architecture/#type-conversion","title":"Type Conversion","text":"<p>The executor automatically converts input values for function parameters: <pre><code>if param_type == float and not isinstance(value, float):\n    converted_value = float(value)\nelif param_type == int and not isinstance(value, (int, bool)):\n    converted_value = int(value)\nelif param_type == str and not isinstance(value, str):\n    converted_value = str(value)\nelif param_type == bool and not isinstance(value, bool):\n    converted_value = bool(value)\n</code></pre></p>"},{"location":"guides/architecture/#llm-message-formatting","title":"LLM Message Formatting","text":"<p>Variable nodes support special formatting for LLM integration:</p> <p>Standard LLM Message Format (<code>llmMessageFormat=true</code>): <pre><code>{\n  \"role\": \"user\",\n  \"content\": \"Hello, world!\"\n}\n</code></pre></p> <p>Image Message Format: <pre><code>{\n  \"role\": \"user\",\n  \"content\": [\n    {\n      \"type\": \"image_url\",\n      \"image_url\": {\"url\": \"data:image/png;base64,...\"}\n    }\n  ]\n}\n</code></pre></p> <p>Legacy Text Content Format (<code>textContentFormat=true</code>): <pre><code>{\n  \"type\": \"text\",\n  \"content\": \"Hello, world!\"\n}\n</code></pre></p>"},{"location":"guides/architecture/#progress-node-pattern","title":"Progress Node Pattern","text":"<p>Progress nodes enable long-running operations to report their status in real-time:</p>"},{"location":"guides/architecture/#implementation-pattern","title":"Implementation Pattern","text":"<pre><code># In nodepacks/&lt;nodepack&gt;/progress_ops.py\n\nclass _ProgressReporter:\n    \"\"\"Internal class for progress reporting.\"\"\"\n    def __init__(self):\n        self.callback = None\n\n    def set_callback(self, callback):\n        self.callback = callback\n\n    def update(self, current: int, total: int, message: str):\n        \"\"\"Report progress (0.0-1.0) with optional message.\"\"\"\n        if self.callback:\n            percent = current / total if total &gt; 0 else 0.0\n            self.callback(percent, message)\n\n\nclass MyProgressOperation:\n    \"\"\"\n    A progress-aware operation that reports execution status.\n    \"\"\"\n    def __init__(self):\n        self._progress_reporter = _ProgressReporter()\n\n    def __call__(self, iterations: int) -&gt; str:\n        \"\"\"Execute operation with progress reporting.\"\"\"\n        for i in range(iterations):\n            # Do work...\n            time.sleep(0.5)\n\n            # Report progress\n            self._progress_reporter.update(\n                current=i + 1,\n                total=iterations,\n                message=f\"Processing iteration {i + 1}/{iterations}\"\n            )\n\n        return f\"Completed {iterations} iterations\"\n</code></pre>"},{"location":"guides/architecture/#threading-model","title":"Threading Model","text":"<ul> <li>Progress nodes execute in separate threads to avoid blocking</li> <li>Progress updates sent via queue from thread to main executor</li> <li>Main executor yields SSE events while thread is running</li> <li>Errors caught in thread and reported via error status</li> </ul>"},{"location":"guides/architecture/#frontend-display","title":"Frontend Display","text":"<ul> <li>Progress nodes show animated progress bar in StatusPanel</li> <li>Progress percentage (0-100%) displayed</li> <li>Progress message shown below bar</li> <li>Real-time updates as operation executes</li> </ul>"},{"location":"guides/architecture/#stream-node-pattern","title":"Stream Node Pattern","text":"<p>Stream nodes enable real-time text streaming for operations like LLM token generation:</p>"},{"location":"guides/architecture/#implementation-pattern_1","title":"Implementation Pattern","text":"<pre><code># In nodepacks/&lt;nodepack&gt;/stream_ops.py\n\nclass _StreamReporter:\n    \"\"\"Internal class for stream reporting.\"\"\"\n    def __init__(self):\n        self._callback = None\n\n    def set_callback(self, callback):\n        \"\"\"Set the callback for stream updates.\"\"\"\n        self._callback = callback\n\n    def emit(self, chunk: str):\n        \"\"\"Emit a text chunk to the stream.\"\"\"\n        if self._callback and chunk:\n            self._callback(chunk)\n\n\nclass MyStreamOperation:\n    \"\"\"\n    A stream-aware operation that emits text chunks in real-time.\n    \"\"\"\n    def __init__(self):\n        self._stream_reporter = _StreamReporter()\n\n    def __call__(self, prompt: str) -&gt; str:\n        \"\"\"Execute operation with streaming output.\"\"\"\n        result_chunks = []\n\n        # Example: Process and emit chunks\n        for word in prompt.split():\n            processed = word.upper() + \" \"\n            result_chunks.append(processed)\n            # Emit each chunk as it's processed\n            self._stream_reporter.emit(processed)\n\n        return \"\".join(result_chunks)\n</code></pre>"},{"location":"guides/architecture/#real-world-example-llm-streaming","title":"Real-World Example: LLM Streaming","text":"<pre><code># In nodepacks/llms/stream_ops.py\n\nclass OpenAIChatCompletionStream:\n    \"\"\"Make a streaming chat completion request to OpenAI.\"\"\"\n\n    def __init__(self):\n        self._stream_reporter = _StreamReporter()\n\n    def __call__(self, model: str, messages: list[dict]) -&gt; dict:\n        from openai import OpenAI\n        client = OpenAI()\n\n        stream = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            stream=True,\n        )\n\n        accumulated_content = []\n        for chunk in stream:\n            if chunk.choices and chunk.choices[0].delta.content:\n                content = chunk.choices[0].delta.content\n                accumulated_content.append(content)\n                # Emit token as it arrives\n                self._stream_reporter.emit(content)\n\n        return {\"content\": \"\".join(accumulated_content)}\n</code></pre>"},{"location":"guides/architecture/#threading-model_1","title":"Threading Model","text":"<ul> <li>Stream nodes execute in separate threads to avoid blocking</li> <li>Text chunks sent via queue from thread to main executor</li> <li>Main executor yields SSE events while thread is running</li> <li>Accumulated text tracked for each streaming update</li> <li>Errors caught in thread and reported via error status</li> </ul>"},{"location":"guides/architecture/#frontend-display_1","title":"Frontend Display","text":"<ul> <li>Stream nodes show real-time text in StatusPanel</li> <li>Accumulated text displayed as it arrives</li> <li>Typing effect as new chunks append</li> <li>Final output shown when streaming completes</li> </ul>"},{"location":"guides/architecture/#error-handling_1","title":"Error Handling","text":""},{"location":"guides/architecture/#backend-errors","title":"Backend Errors","text":"<p>Schema Extraction: - Module loading failures (invalid Python syntax, missing dependencies) - Missing type hints (falls back to <code>Any</code> type) - Invalid class structures (classes without <code>__call__</code> ignored)</p> <p>Graph Execution: - Cycle detection via topological sort (raises <code>ValueError</code>) - Missing functions/classes in registries (node skipped, output set to <code>None</code>) - Type conversion errors (caught, logged, output set to <code>None</code>) - Function execution exceptions (caught, logged, yields error status in streaming) - Thread execution errors for progress and stream nodes (caught, yields error status)</p> <p>API Errors: - Invalid request format (400 Bad Request) - Execution failures (500 Internal Server Error with error details) - SSE stream errors (error event with status: \"error\")</p>"},{"location":"guides/architecture/#frontend-errors","title":"Frontend Errors","text":"<p>Schema Loading: - Network errors (retries with exponential backoff via useSchema hook) - Invalid response format (empty node library displayed) - Backend unreachable (error message in console)</p> <p>Graph Execution: - Network errors (execution marked as failed) - Backend failures (error displayed in StatusPanel) - SSE stream interruptions (cleanup via abortExecution callback) - Cycle detection (error event displayed)</p> <p>User Input: - Invalid type conversions (error shown in node) - Missing required parameters (validation in UI) - Disconnected required inputs (executed with <code>None</code> value)</p>"},{"location":"guides/architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/architecture/#backend","title":"Backend","text":"<p>Initialization: - Function, progress class, and stream class registries built once on startup - Module imports cached by Python's import system - Nodepacks directory scanned only during initialization</p> <p>Execution: - Topological sort: O(V + E) complexity where V=nodes, E=edges - Sequential node execution (no parallelization currently) - Type conversion performed once per parameter - Environment variables saved/restored efficiently</p> <p>Streaming: - SSE events sent immediately as nodes complete - No buffering of status updates - Progress updates queued and yielded in real-time - Thread overhead for progress and stream nodes (one thread per progress/stream node)</p>"},{"location":"guides/architecture/#frontend","title":"Frontend","text":"<p>Rendering: - React Flow handles large graphs efficiently (virtualization) - Memoized node components prevent unnecessary re-renders - State updates batched by React for performance</p> <p>Execution Monitoring: - StatusPanel updates incrementally (not full re-render) - SSE stream parsed and processed efficiently - Auto-scroll disabled during user interaction</p> <p>Memory Management: - Previous status history cleared before new execution - SSE streams properly cleaned up via abortExecution - Node components unmount properly on deletion</p>"},{"location":"guides/architecture/#extension-points","title":"Extension Points","text":""},{"location":"guides/architecture/#adding-new-node-types","title":"Adding New Node Types","text":"<p>Backend (<code>psynapse_backend/executor.py</code>): 1. Add new node type handling in <code>execute_graph()</code> and <code>execute_graph_streaming()</code> 2. Implement execution logic for the node type 3. Handle inputs/outputs appropriately 4. Add to node type checks (e.g., <code>elif node_type == \"newNodeType\"</code>)</p> <p>Frontend (<code>frontend/src/components/</code>): 1. Create new node component in <code>frontend/src/components/NewNodeType.tsx</code> 2. Add to <code>nodeTypes</code> registry in <code>PsynapseEditor.tsx</code> 3. Add to NodeLibraryPanel for drag-and-drop 4. Update TypeScript types in <code>frontend/src/types/</code></p>"},{"location":"guides/architecture/#adding-new-data-types","title":"Adding New Data Types","text":"<p>Backend: 1. Update type hints in Python functions (e.g., use <code>MyCustomType</code> annotation) 2. Update <code>get_type_name()</code> in <code>psynapse_backend/schema_extractor.py</code> 3. Add type conversion logic in executor's <code>execute_graph()</code> methods 4. Document the type in schema extraction</p> <p>Frontend: 1. Update input components to handle new type 2. Add UI controls for the type (if needed) 3. Update type conversion in node components 4. Add validation for the new type</p>"},{"location":"guides/architecture/#custom-nodepacks","title":"Custom Nodepacks","text":"<p>Creating a Nodepack: 1. Create directory: <code>nodepacks/my_nodepack/</code> 2. Add <code>ops.py</code> with type-hinted functions:    <pre><code>def my_function(param1: str, param2: int = 10) -&gt; str:\n    \"\"\"Function description.\"\"\"\n    return f\"{param1} {param2}\"\n</code></pre> 3. (Optional) Add <code>progress_ops.py</code> with progress classes:    <pre><code>class MyProgressOp:\n    def __init__(self):\n        self._progress_reporter = _ProgressReporter()\n\n    def __call__(self, iterations: int) -&gt; str:\n        for i in range(iterations):\n            # Work...\n            self._progress_reporter.update(i+1, iterations, f\"Step {i+1}\")\n        return \"Done\"\n</code></pre> 4. (Optional) Add <code>stream_ops.py</code> with stream classes:    <pre><code>class MyStreamOp:\n    def __init__(self):\n        self._stream_reporter = _StreamReporter()\n\n    def __call__(self, text: str) -&gt; str:\n        result = []\n        for word in text.split():\n            self._stream_reporter.emit(word + \" \")\n            result.append(word)\n        return \" \".join(result)\n</code></pre> 5. Restart backend or use <code>--nodepack-dir</code> flag 5. Functions/classes automatically appear in library</p> <p>Best Practices: - Always include type hints for all parameters and return values - Use <code>Literal</code> types for dropdown selections - Provide clear docstrings (shown in UI tooltips) - Use default values for optional parameters - Test functions independently before integration</p>"},{"location":"guides/architecture/#testing","title":"Testing","text":""},{"location":"guides/architecture/#backend-tests","title":"Backend Tests","text":"<p>See <code>psynapse_backend/test_backend.py</code> for examples:</p> <p>Schema Extraction: - Verify correct parameter extraction - Test default value handling - Validate <code>Literal</code> type detection - Check progress class, stream class, and regular function differentiation</p> <p>Graph Execution: - Test topological sort with various graph structures - Verify cycle detection (should raise <code>ValueError</code>) - Test all node types: FunctionNode, VariableNode, ListNode, ViewNode - Validate type conversion accuracy - Test progress and stream node threading and callbacks - Verify environment variable injection and restoration</p> <p>Integration Tests: - End-to-end workflow execution - Multi-node graph with complex dependencies - Error handling and recovery</p>"},{"location":"guides/architecture/#frontend-tests-recommended","title":"Frontend Tests (Recommended)","text":"<p>Component Tests: - Node component rendering (FunctionNode, VariableNode, ListNode, ViewNode) - StatusPanel updates with different status types - NodeLibraryPanel schema loading</p> <p>Integration Tests: - Drag-and-drop functionality - Edge creation and validation - Graph execution flow with SSE - Error display and handling</p> <p>E2E Tests: - Complete workflow: load \u2192 edit \u2192 execute \u2192 view results - Progress and stream node real-time updates - Abort execution during progress</p>"},{"location":"guides/architecture/#deployment","title":"Deployment","text":""},{"location":"guides/architecture/#docker-compose-recommended","title":"Docker Compose (Recommended)","text":"<pre><code># Build and start all services\ndocker compose -f docker/docker-compose.yml up --build\n\n# With optional LLM support\nOPTIONAL_DEPS=\"llm\" docker compose -f docker/docker-compose.yml up --build\n\n# Detached mode\ndocker compose -f docker/docker-compose.yml up -d\n</code></pre>"},{"location":"guides/architecture/#backend-deployment","title":"Backend Deployment","text":"<p>Development: <pre><code>psynapse-backend run --host 0.0.0.0 --port 8000 --reload\n</code></pre></p> <p>Production: <pre><code># Using Uvicorn with multiple workers\nuvicorn psynapse_backend.main:app --host 0.0.0.0 --port 8000 --workers 4\n\n# Or via CLI\npsynapse-backend run --host 0.0.0.0 --port 8000\n</code></pre></p> <p>Environment Variables: - <code>NODEPACKS_DIR</code>: Path to nodepacks directory (default: <code>./nodepacks</code>) - <code>CORS_ORIGINS</code>: Comma-separated allowed origins (production)</p>"},{"location":"guides/architecture/#frontend-deployment","title":"Frontend Deployment","text":"<p>Development: <pre><code>cd frontend\nnpm install\nnpm run dev\n</code></pre></p> <p>Production Build: <pre><code>cd frontend\nnpm run build\nnpm run preview  # Test production build\n\n# Deploy dist/ directory with nginx/apache/CDN\n</code></pre></p> <p>Environment Variables: - <code>VITE_API_URL</code>: Backend API URL (default: <code>http://localhost:8000</code>)</p>"},{"location":"guides/architecture/#nginx-configuration-example","title":"Nginx Configuration Example","text":"<pre><code>server {\n    listen 80;\n    server_name your-domain.com;\n\n    # Frontend\n    location / {\n        root /var/www/psynapse/frontend/dist;\n        try_files $uri $uri/ /index.html;\n    }\n\n    # Backend API\n    location /api/ {\n        proxy_pass http://localhost:8000/;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n\n        # For SSE streaming\n        proxy_buffering off;\n        proxy_cache off;\n    }\n}\n</code></pre>"},{"location":"guides/architecture/#future-enhancements","title":"Future Enhancements","text":""},{"location":"guides/architecture/#planned-features","title":"Planned Features","text":"<ol> <li>Workflow Persistence:</li> <li>Save/load workflows to JSON files</li> <li>Workflow versioning</li> <li> <p>Template library</p> </li> <li> <p>Advanced Execution:</p> </li> <li>Parallel node execution (where possible)</li> <li>Conditional execution (if/else nodes)</li> <li>Loop nodes (iterate over lists)</li> <li> <p>Subgraph nodes (reusable components)</p> </li> <li> <p>Enhanced UI:</p> </li> <li>Undo/redo functionality</li> <li>Workflow minimap navigation</li> <li>Node search and filtering</li> <li>Custom node styling</li> <li> <p>Workflow comments and annotations</p> </li> <li> <p>Validation &amp; Debugging:</p> </li> <li>Pre-execution graph validation</li> <li>Type checking before execution</li> <li>Step-through debugging</li> <li>Breakpoints on nodes</li> <li> <p>Variable inspection</p> </li> <li> <p>Advanced Types:</p> </li> <li>Tensor/array types (NumPy, PyTorch)</li> <li>DataFrame support (Pandas)</li> <li>File path types with file picker</li> <li> <p>Custom type validators</p> </li> <li> <p>Performance:</p> </li> <li>Node result caching/memoization</li> <li>Incremental execution (only re-run changed nodes)</li> <li>Async execution for I/O-bound operations</li> <li> <p>WebSocket alternative to SSE</p> </li> <li> <p>Collaboration:</p> </li> <li>Multi-user editing (real-time)</li> <li>Workflow sharing</li> <li>Comment threads on nodes</li> <li> <p>Execution history</p> </li> <li> <p>Export &amp; Integration:</p> </li> <li>Generate standalone Python scripts</li> <li>Export as Jupyter notebook</li> <li>API endpoint generation from workflows</li> <li>CI/CD integration</li> </ol>"},{"location":"guides/architecture/#conclusion","title":"Conclusion","text":"<p>Psynapse provides a flexible, extensible platform for visual programming that combines the power of Python with an intuitive node-based interface. The architecture separates concerns effectively across three main layers:</p> <ul> <li>Backend: Robust execution engine with schema introspection</li> <li>Frontend: Interactive visual editor with real-time feedback</li> <li>Nodepacks: Extensible function library system</li> </ul> <p>Key architectural strengths: - Dynamic Schema Extraction: Functions automatically become nodes - Real-time Execution Monitoring: SSE streaming for progress tracking - Type Safety: Automatic type conversion and validation - Extensibility: Easy to add new node types and nodepacks - Developer Experience: Hot reloading, clear error messages, comprehensive docs</p> <p>The use of Python's introspection capabilities and React Flow's powerful graph editing features creates a seamless user experience for building and executing complex computational workflows.</p>"},{"location":"guides/progress-nodes/","title":"Progress Nodes Guide","text":""},{"location":"guides/progress-nodes/#overview","title":"Overview","text":"<p>Progress nodes are a special type of node in Psynapse that can report real-time progress updates during execution. These nodes are ideal for long-running operations where users need visual feedback about the execution status.</p> <p>Progress nodes are implemented as Python classes with a <code>__call__</code> method, stored in <code>progress_ops.py</code> files within nodepacks. During execution, they report progress updates that are displayed as progress bars in the Status Panel.</p>"},{"location":"guides/progress-nodes/#creating-progress-nodes","title":"Creating Progress Nodes","text":""},{"location":"guides/progress-nodes/#file-structure","title":"File Structure","text":"<p>Progress nodes must be placed in a <code>progress_ops.py</code> file within a nodepack directory:</p> <pre><code>nodepacks/\n  \u2514\u2500\u2500 basic/\n      \u251c\u2500\u2500 ops.py              # Regular function nodes\n      \u2514\u2500\u2500 progress_ops.py     # Progress-aware nodes\n</code></pre>"},{"location":"guides/progress-nodes/#class-structure","title":"Class Structure","text":"<p>A progress node is a Python class that:</p> <ol> <li>Has a <code>__call__</code> method that serves as the entry point</li> <li>Contains a <code>_ProgressReporter</code> instance for reporting progress</li> <li>Calls <code>update()</code> or <code>update_percent()</code> during execution</li> </ol>"},{"location":"guides/progress-nodes/#example-basic-progress-node","title":"Example: Basic Progress Node","text":"<pre><code>import time\nfrom typing import Callable, Optional\n\n\nclass _ProgressReporter:\n    \"\"\"Context-aware progress reporter for node execution.\"\"\"\n\n    def __init__(self):\n        self._callback: Optional[Callable[[float, str], None]] = None\n\n    def set_callback(self, callback: Callable[[float, str], None]):\n        \"\"\"Set the callback for progress updates.\"\"\"\n        self._callback = callback\n\n    def update(self, current: int, total: int, message: str = \"\"):\n        \"\"\"Report progress using current/total.\"\"\"\n        if self._callback:\n            progress = current / total if total &gt; 0 else 0\n            self._callback(progress, message)\n\n    def update_percent(self, percent: float, message: str = \"\"):\n        \"\"\"Report progress as percentage (0.0 to 1.0).\"\"\"\n        if self._callback:\n            self._callback(percent, message)\n\n\nclass ProcessItems:\n    \"\"\"Process a list of items with progress reporting.\"\"\"\n\n    def __init__(self):\n        self._progress_reporter = _ProgressReporter()\n\n    def __call__(self, count: int) -&gt; int:\n        \"\"\"\n        Process items with progress reporting.\n\n        Args:\n            count: Number of items to process\n\n        Returns:\n            Sum of processed items\n        \"\"\"\n        results = []\n        for i in range(count):\n            # Simulate work\n            time.sleep(0.5)\n            results.append(i * 2)\n\n            # Report progress\n            self._progress_reporter.update(\n                i + 1, \n                count, \n                f\"Processing item {i + 1}/{count}\"\n            )\n\n        return sum(results)\n</code></pre>"},{"location":"guides/progress-nodes/#key-components","title":"Key Components","text":""},{"location":"guides/progress-nodes/#1-_progressreporter-class","title":"1. <code>_ProgressReporter</code> Class","text":"<p>The <code>_ProgressReporter</code> class handles progress callbacks:</p> <ul> <li><code>set_callback(callback)</code>: Sets the callback function (called by executor)</li> <li><code>update(current, total, message)</code>: Reports progress using current/total ratio</li> <li><code>update_percent(percent, message)</code>: Reports progress as a percentage (0.0-1.0)</li> </ul>"},{"location":"guides/progress-nodes/#2-progress-node-class","title":"2. Progress Node Class","text":"<p>Your progress node class must:</p> <ul> <li>Have <code>__init__</code>: Create a <code>_ProgressReporter</code> instance</li> <li>Have <code>__call__</code>: Implement the main logic with progress reporting</li> <li>Call <code>update()</code> or <code>update_percent()</code>: Report progress during execution</li> </ul>"},{"location":"guides/progress-nodes/#progress-reporting-methods","title":"Progress Reporting Methods","text":""},{"location":"guides/progress-nodes/#using-updatecurrent-total-message","title":"Using <code>update(current, total, message)</code>","text":"<pre><code>def __call__(self, items: list) -&gt; list:\n    results = []\n    total = len(items)\n\n    for i, item in enumerate(items):\n        # Process item\n        result = process(item)\n        results.append(result)\n\n        # Report progress: automatically calculates percentage\n        self._progress_reporter.update(\n            i + 1,           # Current item number\n            total,           # Total items\n            f\"Processing {item}\"  # Optional message\n        )\n\n    return results\n</code></pre>"},{"location":"guides/progress-nodes/#using-update_percentpercent-message","title":"Using <code>update_percent(percent, message)</code>","text":"<pre><code>def __call__(self, steps: int) -&gt; float:\n    result = 0.0\n\n    for i in range(steps):\n        # Perform calculation\n        result += calculate_step(i)\n\n        # Report progress: manually calculate percentage\n        percent = (i + 1) / steps\n        self._progress_reporter.update_percent(\n            percent,\n            f\"Step {i + 1}/{steps}\"\n        )\n\n    return result\n</code></pre>"},{"location":"guides/progress-nodes/#execution-flow","title":"Execution Flow","text":"<p>When a progress node is executed:</p> <ol> <li>Discovery: Schema extractor finds the class in <code>progress_ops.py</code></li> <li>Registration: Class is registered in <code>progress_class_registry</code></li> <li>Instantiation: Executor creates a new instance of the class</li> <li>Callback Setup: Executor sets the progress callback on <code>_progress_reporter</code></li> <li>Thread Execution: <code>__call__</code> method runs in a separate thread</li> <li>Progress Streaming: Progress updates are streamed via Server-Sent Events</li> <li>UI Update: Status Panel displays progress bar with percentage</li> </ol>"},{"location":"guides/progress-nodes/#frontend-display","title":"Frontend Display","text":"<p>Progress nodes display in the Status Panel with:</p> <ul> <li>Progress Bar: Visual indicator showing completion percentage</li> <li>Progress Message: Custom message from the node</li> <li>Percentage Text: Numeric percentage display</li> <li>Spinner: Animated indicator showing active execution</li> </ul> <p>The progress bar updates in real-time as the node reports progress.</p>"},{"location":"guides/progress-nodes/#best-practices","title":"Best Practices","text":""},{"location":"guides/progress-nodes/#1-report-progress-frequently","title":"1. Report Progress Frequently","text":"<p>Report progress at regular intervals, especially for long-running operations:</p> <pre><code># Good: Report every iteration\nfor i, item in enumerate(items):\n    process(item)\n    self._progress_reporter.update(i + 1, len(items))\n\n# Bad: Report only at the end\nfor item in items:\n    process(item)\nself._progress_reporter.update(len(items), len(items))  # Too late!\n</code></pre>"},{"location":"guides/progress-nodes/#2-use-descriptive-messages","title":"2. Use Descriptive Messages","text":"<p>Provide meaningful progress messages:</p> <pre><code># Good\nself._progress_reporter.update(\n    i + 1, \n    total, \n    f\"Processing file: {filename}\"\n)\n\n# Bad\nself._progress_reporter.update(i + 1, total, \"\")\n</code></pre>"},{"location":"guides/progress-nodes/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<p>Ensure progress reporting doesn't break error handling:</p> <pre><code>def __call__(self, items: list) -&gt; list:\n    results = []\n    try:\n        for i, item in enumerate(items):\n            result = process(item)\n            results.append(result)\n            self._progress_reporter.update(i + 1, len(items))\n    except Exception as e:\n        # Error will be caught by executor\n        raise\n    return results\n</code></pre>"},{"location":"guides/progress-nodes/#4-consider-performance","title":"4. Consider Performance","text":"<p>Progress reporting has minimal overhead, but avoid excessive updates:</p> <pre><code># Good: Report every N items for large datasets\nfor i, item in enumerate(items):\n    process(item)\n    if i % 100 == 0 or i == len(items) - 1:\n        self._progress_reporter.update(i + 1, len(items))\n\n# Also fine: Report every item for smaller datasets\nfor i, item in enumerate(items):\n    process(item)\n    self._progress_reporter.update(i + 1, len(items))\n</code></pre>"},{"location":"guides/progress-nodes/#advanced-examples","title":"Advanced Examples","text":""},{"location":"guides/progress-nodes/#example-file-processing-with-nested-progress","title":"Example: File Processing with Nested Progress","text":"<pre><code>class ProcessFiles:\n    def __init__(self):\n        self._progress_reporter = _ProgressReporter()\n\n    def __call__(self, files: list[str]) -&gt; dict:\n        results = {}\n        total_files = len(files)\n\n        for file_idx, filename in enumerate(files):\n            # Report file-level progress\n            self._progress_reporter.update(\n                file_idx + 1,\n                total_files,\n                f\"Processing file: {filename}\"\n            )\n\n            # Process file with line-level progress\n            file_results = []\n            lines = read_file(filename)\n            for line_idx, line in enumerate(lines):\n                processed = process_line(line)\n                file_results.append(processed)\n\n                # Report line-level progress\n                self._progress_reporter.update_percent(\n                    (file_idx + (line_idx + 1) / len(lines)) / total_files,\n                    f\"File {file_idx + 1}/{total_files}: Line {line_idx + 1}/{len(lines)}\"\n                )\n\n            results[filename] = file_results\n\n        return results\n</code></pre>"},{"location":"guides/progress-nodes/#example-iterative-algorithm-with-progress","title":"Example: Iterative Algorithm with Progress","text":"<pre><code>class IterativeSolver:\n    def __init__(self):\n        self._progress_reporter = _ProgressReporter()\n\n    def __call__(self, iterations: int, tolerance: float) -&gt; float:\n        value = 0.0\n\n        for i in range(iterations):\n            # Perform iteration\n            value = iterate(value)\n\n            # Check convergence\n            if abs(value - previous_value) &lt; tolerance:\n                self._progress_reporter.update_percent(\n                    1.0,\n                    f\"Converged after {i + 1} iterations\"\n                )\n                break\n\n            # Report progress\n            self._progress_reporter.update(\n                i + 1,\n                iterations,\n                f\"Iteration {i + 1}: value = {value:.6f}\"\n            )\n\n        return value\n</code></pre>"},{"location":"guides/progress-nodes/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/progress-nodes/#progress-bar-not-appearing","title":"Progress Bar Not Appearing","text":"<ul> <li>Ensure your class has <code>__call__</code> method</li> <li>Verify <code>_progress_reporter</code> is created in <code>__init__</code></li> <li>Check that <code>update()</code> or <code>update_percent()</code> is called during execution</li> <li>Ensure the class is in <code>progress_ops.py</code> (not <code>ops.py</code>)</li> </ul>"},{"location":"guides/progress-nodes/#progress-updates-not-showing","title":"Progress Updates Not Showing","text":"<ul> <li>Check that progress is reported frequently enough</li> <li>Verify the executor is using streaming mode (<code>/execute/stream</code>)</li> <li>Check browser console for SSE connection errors</li> </ul>"},{"location":"guides/progress-nodes/#class-not-discovered","title":"Class Not Discovered","text":"<ul> <li>Ensure class name doesn't start with <code>_</code> (private classes are skipped)</li> <li>Verify file is named <code>progress_ops.py</code> exactly</li> <li>Check that class is defined at module level (not nested)</li> <li>Restart backend after adding new progress nodes</li> </ul>"},{"location":"guides/progress-nodes/#api-reference","title":"API Reference","text":""},{"location":"guides/progress-nodes/#_progressreporter-methods","title":"<code>_ProgressReporter</code> Methods","text":""},{"location":"guides/progress-nodes/#updatecurrent-int-total-int-message-str","title":"<code>update(current: int, total: int, message: str = \"\")</code>","text":"<p>Report progress using current item and total items.</p> <ul> <li>current: Current item number (1-indexed)</li> <li>total: Total number of items</li> <li>message: Optional progress message</li> </ul>"},{"location":"guides/progress-nodes/#update_percentpercent-float-message-str","title":"<code>update_percent(percent: float, message: str = \"\")</code>","text":"<p>Report progress as a percentage.</p> <ul> <li>percent: Progress percentage (0.0 to 1.0)</li> <li>message: Optional progress message</li> </ul>"},{"location":"guides/progress-nodes/#schema-format","title":"Schema Format","text":"<p>Progress nodes are included in the schema with <code>is_progress_node: true</code>:</p> <pre><code>{\n  \"name\": \"ProcessItems\",\n  \"params\": [\n    {\"name\": \"count\", \"type\": \"int\"}\n  ],\n  \"returns\": [\n    {\"name\": \"result\", \"type\": \"int\"}\n  ],\n  \"docstring\": \"Process items with progress reporting.\",\n  \"filepath\": \"/path/to/progress_ops.py\",\n  \"is_progress_node\": true\n}\n</code></pre>"},{"location":"guides/progress-nodes/#see-also","title":"See Also","text":"<ul> <li>Architecture Guide - System architecture overview</li> <li>Graph Executor - Execution details</li> <li>Schema Extractor - Schema extraction process</li> </ul>"},{"location":"guides/quick-start/","title":"Psynapse Quick Start Guide","text":""},{"location":"guides/quick-start/#get-started-in-2-ways","title":"\ud83d\ude80 Get Started in 2 Ways","text":""},{"location":"guides/quick-start/#option-1-docker-compose-recommended","title":"Option 1: Docker Compose (Recommended)","text":"<p>The fastest way to get Psynapse running:</p> <pre><code>git clone https://github.com/soumik12345/psynapse\ncd psynapse\ndocker compose -f docker/docker-compose.yml up --build\n</code></pre> <p>Access the editor at http://localhost:5173</p> <p>Optional: With LLM Support</p> <p>To enable LLM nodepacks (requires additional dependencies):</p> <pre><code>OPTIONAL_DEPS=llm docker compose -f docker/docker-compose.yml up --build\n</code></pre>"},{"location":"guides/quick-start/#option-2-local-development","title":"Option 2: Local Development","text":"<p>For development and customization:</p>"},{"location":"guides/quick-start/#step-1-clone-and-install","title":"Step 1: Clone and Install","text":"<pre><code>git clone https://github.com/soumik12345/psynapse\ncd psynapse\n</code></pre>"},{"location":"guides/quick-start/#step-2-start-the-backend","title":"Step 2: Start the Backend","text":"<pre><code>uv sync\npsynapse-backend run --reload\n</code></pre> <p>You should see: <pre><code>INFO:     Started server process\nINFO:     Uvicorn running on http://0.0.0.0:8000\n</code></pre></p> <p>Optional: Install with LLM support</p> <pre><code>uv sync --extra llm\n</code></pre>"},{"location":"guides/quick-start/#step-3-start-the-frontend","title":"Step 3: Start the Frontend","text":"<p>Open another terminal and run:</p> <pre><code>cd frontend\nnpm install\nnpm run dev\n</code></pre> <p>You should see: <pre><code>VITE ready in XXX ms\n\u279c  Local:   http://localhost:5173/\n</code></pre></p>"},{"location":"guides/quick-start/#step-4-open-your-browser","title":"Step 4: Open Your Browser","text":"<p>Navigate to: http://localhost:5173</p>"},{"location":"guides/quick-start/#first-workflow","title":"\ud83c\udfaf First Workflow","text":"<p>Create your first workflow in 30 seconds:</p> <ol> <li>Drag an \"add\" node from the left panel onto the canvas</li> <li>Enter values in the node: <code>a=10</code>, <code>b=5</code></li> <li>Drag a \"ViewNode\" onto the canvas</li> <li>Connect the output (right side) of the add node to the input (left side) of the ViewNode</li> <li>Click \"Execute\" button (top-right)</li> <li>See the result: ViewNode shows <code>15</code></li> </ol>"},{"location":"guides/quick-start/#tips","title":"\ud83d\udcdd Tips","text":"<ul> <li>Connect nodes: Drag from output socket (right) to input socket (left)</li> <li>Input values: Type directly or connect from another node</li> <li>Multiple ViewNodes: Add multiple ViewNodes to inspect different parts of your workflow</li> <li>Minimap: Use the minimap (bottom-right) to navigate large workflows</li> <li>Canvas controls: Scroll to zoom, drag to pan</li> </ul>"},{"location":"guides/quick-start/#example-calculator","title":"\ud83e\uddea Example: Calculator","text":"<p>Create <code>(5 + 3) \u00d7 (2 + 4) = 48</code>:</p> <ol> <li>Drag 2 \"add\" nodes onto canvas</li> <li>First add: <code>a=5</code>, <code>b=3</code></li> <li>Second add: <code>a=2</code>, <code>b=4</code></li> <li>Drag 1 \"multiply\" node onto canvas</li> <li>Connect:</li> <li>First add output \u2192 multiply's <code>a</code> input</li> <li>Second add output \u2192 multiply's <code>b</code> input</li> <li>Drag ViewNode and connect multiply output to it</li> <li>Click Execute</li> <li>Result: 48</li> </ol>"},{"location":"guides/quick-start/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"guides/quick-start/#backend-wont-start","title":"Backend won't start","text":"<pre><code>cd backend\nuv venv\nsource .venv/bin/activate\nuv pip install -r requirements.txt\nuv run python main.py\n</code></pre>"},{"location":"guides/quick-start/#frontend-wont-start","title":"Frontend won't start","text":"<pre><code>cd frontend\nnpm install\nnpm run dev\n</code></pre>"},{"location":"guides/stream-nodes/","title":"Stream Nodes Guide","text":""},{"location":"guides/stream-nodes/#overview","title":"Overview","text":"<p>Stream nodes are a special type of node in Psynapse that can emit text chunks in real-time during execution. These nodes are ideal for operations that produce incremental text output, such as LLM token streaming, where users want to see the response as it's being generated.</p> <p>Stream nodes are implemented as Python classes with a <code>__call__</code> method, stored in <code>stream_ops.py</code> files within nodepacks. During execution, they emit text chunks that are displayed in real-time in the Status Panel.</p>"},{"location":"guides/stream-nodes/#creating-stream-nodes","title":"Creating Stream Nodes","text":""},{"location":"guides/stream-nodes/#file-structure","title":"File Structure","text":"<p>Stream nodes must be placed in a <code>stream_ops.py</code> file within a nodepack directory:</p> <pre><code>nodepacks/\n  \u2514\u2500\u2500 my_nodepack/\n      \u251c\u2500\u2500 ops.py              # Regular function nodes\n      \u251c\u2500\u2500 progress_ops.py     # Progress-aware nodes\n      \u2514\u2500\u2500 stream_ops.py       # Streaming text nodes\n</code></pre>"},{"location":"guides/stream-nodes/#class-structure","title":"Class Structure","text":"<p>A stream node is a Python class that:</p> <ol> <li>Has a <code>__call__</code> method that serves as the entry point</li> <li>Contains a <code>_StreamReporter</code> instance for emitting text chunks</li> <li>Calls <code>emit()</code> during execution to stream text</li> </ol>"},{"location":"guides/stream-nodes/#example-basic-stream-node","title":"Example: Basic Stream Node","text":"<pre><code>from typing import Callable, Optional\n\n\nclass _StreamReporter:\n    \"\"\"Context-aware stream reporter for streaming text content.\"\"\"\n\n    def __init__(self):\n        self._callback: Optional[Callable[[str], None]] = None\n\n    def set_callback(self, callback: Callable[[str], None]):\n        \"\"\"Set the callback for stream updates.\"\"\"\n        self._callback = callback\n\n    def emit(self, chunk: str):\n        \"\"\"Emit a text chunk to the stream.\"\"\"\n        if self._callback and chunk:\n            self._callback(chunk)\n\n\nclass StreamText:\n    \"\"\"Stream text word by word.\"\"\"\n\n    def __init__(self):\n        self._stream_reporter = _StreamReporter()\n\n    def __call__(self, text: str, delay: float = 0.1) -&gt; str:\n        \"\"\"\n        Stream text content word by word.\n\n        Args:\n            text: The text to stream\n            delay: Delay between words in seconds\n\n        Returns:\n            The complete text\n        \"\"\"\n        import time\n\n        words = text.split()\n        for i, word in enumerate(words):\n            time.sleep(delay)\n            # Add space after word (except for last word)\n            chunk = word + (\" \" if i &lt; len(words) - 1 else \"\")\n            self._stream_reporter.emit(chunk)\n\n        return text\n</code></pre>"},{"location":"guides/stream-nodes/#key-components","title":"Key Components","text":""},{"location":"guides/stream-nodes/#1-_streamreporter-class","title":"1. <code>_StreamReporter</code> Class","text":"<p>The <code>_StreamReporter</code> class handles streaming callbacks:</p> <ul> <li><code>set_callback(callback)</code>: Sets the callback function (called by executor)</li> <li><code>emit(chunk)</code>: Emits a text chunk to be displayed in real-time</li> </ul>"},{"location":"guides/stream-nodes/#2-stream-node-class","title":"2. Stream Node Class","text":"<p>Your stream node class must:</p> <ul> <li>Have <code>__init__</code>: Create a <code>_StreamReporter</code> instance</li> <li>Have <code>__call__</code>: Implement the main logic with stream emission</li> <li>Call <code>emit()</code>: Emit text chunks during execution</li> </ul>"},{"location":"guides/stream-nodes/#streaming-method","title":"Streaming Method","text":""},{"location":"guides/stream-nodes/#using-emitchunk","title":"Using <code>emit(chunk)</code>","text":"<pre><code>def __call__(self, prompt: str) -&gt; str:\n    response_parts = []\n\n    for token in generate_tokens(prompt):\n        # Emit token immediately for real-time display\n        self._stream_reporter.emit(token)\n        response_parts.append(token)\n\n    # Return the complete response\n    return \"\".join(response_parts)\n</code></pre>"},{"location":"guides/stream-nodes/#execution-flow","title":"Execution Flow","text":"<p>When a stream node is executed:</p> <ol> <li>Discovery: Schema extractor finds the class in <code>stream_ops.py</code></li> <li>Registration: Class is registered in <code>stream_class_registry</code></li> <li>Instantiation: Executor creates a new instance of the class</li> <li>Callback Setup: Executor sets the stream callback on <code>_stream_reporter</code></li> <li>Thread Execution: <code>__call__</code> method runs in a separate thread</li> <li>Chunk Streaming: Text chunks are streamed via Server-Sent Events (SSE)</li> <li>UI Update: Status Panel displays accumulated text in real-time</li> </ol>"},{"location":"guides/stream-nodes/#frontend-display","title":"Frontend Display","text":"<p>Stream nodes display in the Status Panel with:</p> <ul> <li>Streaming Text: Accumulated text shown as it arrives</li> <li>Real-time Updates: Each chunk appends to the display immediately</li> <li>Spinner: Animated indicator showing active streaming</li> <li>Final Result: Complete output shown when streaming completes</li> </ul> <p>The text updates in real-time as the node emits chunks, providing immediate feedback.</p>"},{"location":"guides/stream-nodes/#best-practices","title":"Best Practices","text":""},{"location":"guides/stream-nodes/#1-emit-meaningful-chunks","title":"1. Emit Meaningful Chunks","text":"<p>Emit chunks at natural boundaries (tokens, words, sentences):</p> <pre><code># Good: Emit complete tokens/words\nfor token in tokenize(text):\n    self._stream_reporter.emit(token)\n\n# Bad: Emit individual characters (too granular)\nfor char in text:\n    self._stream_reporter.emit(char)\n</code></pre>"},{"location":"guides/stream-nodes/#2-handle-empty-chunks","title":"2. Handle Empty Chunks","text":"<p>The <code>_StreamReporter.emit()</code> method ignores empty chunks, but it's good practice to check:</p> <pre><code># Good: The reporter handles this, but explicit checks are clearer\nfor chunk in generate_chunks():\n    if chunk:\n        self._stream_reporter.emit(chunk)\n</code></pre>"},{"location":"guides/stream-nodes/#3-return-complete-output","title":"3. Return Complete Output","text":"<p>Always return the complete output from <code>__call__</code>, not just the last chunk:</p> <pre><code>def __call__(self, input_text: str) -&gt; str:\n    accumulated = []\n\n    for chunk in process(input_text):\n        self._stream_reporter.emit(chunk)\n        accumulated.append(chunk)\n\n    # Return the complete result\n    return \"\".join(accumulated)\n</code></pre>"},{"location":"guides/stream-nodes/#4-handle-errors-gracefully","title":"4. Handle Errors Gracefully","text":"<p>Ensure streaming doesn't break error handling:</p> <pre><code>def __call__(self, prompt: str) -&gt; str:\n    accumulated = []\n    try:\n        for chunk in generate_response(prompt):\n            self._stream_reporter.emit(chunk)\n            accumulated.append(chunk)\n    except Exception as e:\n        # Error will be caught by executor\n        raise\n    return \"\".join(accumulated)\n</code></pre>"},{"location":"guides/stream-nodes/#5-consider-chunk-size","title":"5. Consider Chunk Size","text":"<p>For optimal UX, balance chunk size with update frequency:</p> <pre><code># Good: Natural chunk sizes (tokens, words)\nfor token in llm_stream:\n    self._stream_reporter.emit(token)\n\n# Good: Buffered output for very fast streams\nbuffer = []\nfor char in fast_stream:\n    buffer.append(char)\n    if len(buffer) &gt;= 10 or char in '.!?\\n':\n        self._stream_reporter.emit(\"\".join(buffer))\n        buffer = []\n</code></pre>"},{"location":"guides/stream-nodes/#advanced-examples","title":"Advanced Examples","text":""},{"location":"guides/stream-nodes/#example-llm-chat-completion-streaming","title":"Example: LLM Chat Completion Streaming","text":"<pre><code>import os\nfrom typing import Any, Callable, Literal, Optional\n\n\nclass _StreamReporter:\n    \"\"\"Context-aware stream reporter for streaming text content.\"\"\"\n\n    def __init__(self):\n        self._callback: Optional[Callable[[str], None]] = None\n\n    def set_callback(self, callback: Callable[[str], None]):\n        \"\"\"Set the callback for stream updates.\"\"\"\n        self._callback = callback\n\n    def emit(self, chunk: str):\n        \"\"\"Emit a text chunk to the stream.\"\"\"\n        if self._callback and chunk:\n            self._callback(chunk)\n\n\nclass OpenAIChatCompletionStream:\n    \"\"\"\n    Make a streaming chat completion request to OpenAI.\n\n    This node streams tokens in real-time to the Execution Status panel,\n    allowing you to see the response as it's being generated.\n    \"\"\"\n\n    def __init__(self):\n        self._stream_reporter = _StreamReporter()\n\n    def __call__(\n        self,\n        model: str,\n        messages: list[dict[str, str]],\n        api_key_variable: str = \"OPENAI_API_KEY\",\n        temperature: float | None = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Make a streaming chat completion request.\n\n        Args:\n            model: The model to use (e.g., \"gpt-4\")\n            messages: The conversation messages\n            api_key_variable: Environment variable containing API key\n            temperature: Sampling temperature (0-2)\n\n        Returns:\n            Complete response dict with message content\n        \"\"\"\n        from openai import OpenAI\n\n        api_key = os.getenv(api_key_variable)\n        if not api_key:\n            raise ValueError(f\"Environment variable '{api_key_variable}' not set\")\n\n        client = OpenAI(api_key=api_key)\n\n        # Make streaming request\n        stream = client.chat.completions.create(\n            model=model,\n            messages=messages,\n            stream=True,\n            temperature=temperature,\n        )\n\n        # Accumulate and stream the response\n        accumulated_content = []\n\n        for chunk in stream:\n            if chunk.choices:\n                for choice in chunk.choices:\n                    if choice.delta and choice.delta.content:\n                        content = choice.delta.content\n                        accumulated_content.append(content)\n                        # Emit chunk for real-time display\n                        self._stream_reporter.emit(content)\n\n        full_content = \"\".join(accumulated_content)\n\n        return {\n            \"model\": model,\n            \"choices\": [{\n                \"message\": {\n                    \"role\": \"assistant\",\n                    \"content\": full_content,\n                }\n            }]\n        }\n</code></pre>"},{"location":"guides/stream-nodes/#example-file-reader-with-streaming-output","title":"Example: File Reader with Streaming Output","text":"<pre><code>class StreamFileContents:\n    \"\"\"Stream file contents line by line.\"\"\"\n\n    def __init__(self):\n        self._stream_reporter = _StreamReporter()\n\n    def __call__(self, filepath: str) -&gt; str:\n        \"\"\"\n        Read and stream file contents line by line.\n\n        Args:\n            filepath: Path to the file to read\n\n        Returns:\n            Complete file contents\n        \"\"\"\n        lines = []\n\n        with open(filepath, 'r') as f:\n            for line in f:\n                self._stream_reporter.emit(line)\n                lines.append(line)\n\n        return \"\".join(lines)\n</code></pre>"},{"location":"guides/stream-nodes/#example-text-transformation-with-streaming","title":"Example: Text Transformation with Streaming","text":"<pre><code>class StreamTransform:\n    \"\"\"Transform text and stream the result.\"\"\"\n\n    def __init__(self):\n        self._stream_reporter = _StreamReporter()\n\n    def __call__(self, text: str, transform: str = \"uppercase\") -&gt; str:\n        \"\"\"\n        Transform text and stream the output.\n\n        Args:\n            text: Input text to transform\n            transform: Transformation type (uppercase, lowercase, title)\n\n        Returns:\n            Transformed text\n        \"\"\"\n        import time\n\n        # Split into sentences for natural streaming\n        sentences = text.replace('. ', '.|').split('|')\n        result_parts = []\n\n        for sentence in sentences:\n            # Apply transformation\n            if transform == \"uppercase\":\n                transformed = sentence.upper()\n            elif transform == \"lowercase\":\n                transformed = sentence.lower()\n            else:\n                transformed = sentence.title()\n\n            # Stream the transformed sentence\n            self._stream_reporter.emit(transformed)\n            result_parts.append(transformed)\n\n            # Small delay for visual effect\n            time.sleep(0.1)\n\n        return \"\".join(result_parts)\n</code></pre>"},{"location":"guides/stream-nodes/#stream-nodes-vs-progress-nodes","title":"Stream Nodes vs Progress Nodes","text":"Feature Stream Nodes Progress Nodes Purpose Emit incremental text output Report completion percentage File <code>stream_ops.py</code> <code>progress_ops.py</code> Reporter <code>_StreamReporter</code> <code>_ProgressReporter</code> Method <code>emit(chunk)</code> <code>update(current, total, message)</code> UI Display Accumulated text Progress bar with percentage Use Case LLM streaming, text generation File processing, batch operations <p>Choose stream nodes when you want to show text as it's generated. Choose progress nodes when you want to show completion status of a multi-step operation.</p>"},{"location":"guides/stream-nodes/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/stream-nodes/#streaming-text-not-appearing","title":"Streaming Text Not Appearing","text":"<ul> <li>Ensure your class has a <code>__call__</code> method</li> <li>Verify <code>_stream_reporter</code> is created in <code>__init__</code></li> <li>Check that <code>emit()</code> is called with non-empty strings</li> <li>Ensure the class is in <code>stream_ops.py</code> (not <code>ops.py</code>)</li> </ul>"},{"location":"guides/stream-nodes/#chunks-not-displaying-in-real-time","title":"Chunks Not Displaying in Real-Time","text":"<ul> <li>Verify the executor is using streaming mode (<code>/execute/stream</code>)</li> <li>Check that chunks are emitted during processing, not all at once at the end</li> <li>Check browser console for SSE connection errors</li> </ul>"},{"location":"guides/stream-nodes/#class-not-discovered","title":"Class Not Discovered","text":"<ul> <li>Ensure class name doesn't start with <code>_</code> (private classes are skipped)</li> <li>Verify file is named <code>stream_ops.py</code> exactly</li> <li>Check that class is defined at module level (not nested)</li> <li>Restart backend after adding new stream nodes</li> </ul>"},{"location":"guides/stream-nodes/#output-not-matching-streamed-text","title":"Output Not Matching Streamed Text","text":"<ul> <li>Ensure <code>__call__</code> returns the complete accumulated text</li> <li>Verify all emitted chunks are also added to the return value</li> </ul>"},{"location":"guides/stream-nodes/#api-reference","title":"API Reference","text":""},{"location":"guides/stream-nodes/#_streamreporter-methods","title":"<code>_StreamReporter</code> Methods","text":""},{"location":"guides/stream-nodes/#emitchunk-str","title":"<code>emit(chunk: str)</code>","text":"<p>Emit a text chunk to the stream.</p> <ul> <li>chunk: Text chunk to emit (empty strings are ignored)</li> </ul>"},{"location":"guides/stream-nodes/#schema-format","title":"Schema Format","text":"<p>Stream nodes are included in the schema with <code>is_stream_node: true</code>:</p> <pre><code>{\n  \"name\": \"StreamText\",\n  \"params\": [\n    {\"name\": \"text\", \"type\": \"str\"},\n    {\"name\": \"delay\", \"type\": \"float\", \"default\": 0.1}\n  ],\n  \"returns\": [\n    {\"name\": \"result\", \"type\": \"str\"}\n  ],\n  \"docstring\": \"Stream text word by word.\",\n  \"filepath\": \"/path/to/stream_ops.py\",\n  \"is_stream_node\": true\n}\n</code></pre>"},{"location":"guides/stream-nodes/#sse-event-format","title":"SSE Event Format","text":"<p>Stream updates are sent via Server-Sent Events with this structure:</p> <pre><code>{\n  \"node_id\": \"node-123\",\n  \"status\": \"streaming\",\n  \"streaming_text\": \"Hello world so far...\",\n  \"streaming_chunk\": \"so far...\"\n}\n</code></pre> <ul> <li>streaming_text: The accumulated text received so far</li> <li>streaming_chunk: The latest chunk that was emitted</li> </ul>"},{"location":"guides/stream-nodes/#see-also","title":"See Also","text":"<ul> <li>Progress Nodes Guide - Progress reporting nodes</li> <li>Architecture Guide - System architecture overview</li> </ul>"}]}